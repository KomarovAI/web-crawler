# AI Integration Guide ü§ñ

–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–æ–∫ —Å –º–∏–Ω–∏–º—É–º–æ–º —Ç–æ–∫–µ–Ω–æ–≤.

## Quick Context (–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–π—Ä–æ–Ω–∫–∏)

```
## WEB CRAWLER PROJECT
- Language: Python 3.11
- Async crawler —Å aiohttp, BeautifulSoup
- Max pages: configurable, default 50
- Domain limitation: –æ–¥–Ω–æ–º–µ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–∏–Ω–≥

## KEY FILES
1. crawler.py - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å Crawler(start_url, max_pages, timeout)
   - async fetch() - –∑–∞–≥—Ä—É–∑–∫–∞ HTML
   - async parse() - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
   - async run() - –≥–ª–∞–≤–Ω—ã–π loop
2. config.py - –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
3. requirements.txt - 3 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

## ARCHITECTURE
Crawler ‚Üí queue-based BFS
  ‚îú‚îÄ fetch(url) ‚Üí HTML|None
  ‚îú‚îÄ parse(html) ‚Üí links[]
  ‚îî‚îÄ validate(url) ‚Üí bool (domain, visited, max_pages)

## ENVIRONMENT
START_URL=https://example.com
MAX_PAGES=50
TIMEOUT=10
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å AI

### –î–ª—è Code Generation
–ö–æ–ø–∏—Ä—É–π —ç—Ç–æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç:
```
–ü—Ä–æ–µ–∫—Ç: Web Crawler –Ω–∞ Python
–†–∞–º–∫–∏: <400 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞, 3 —Ñ–∞–π–ª–∞
–¢–µ—Ö—Å—Ç–µ–∫: aiohttp, BeautifulSoup4, asyncio
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –º–∏–Ω–∏–º—É–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, production-ready, —Ç–∏–ø–∏–∑–∞—Ü–∏—è
```

### –î–ª—è Analysis
```
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ —Ñ–∞–π–ª—ã:
- crawler.py (–∫–ª–∞—Å—Å Crawler, –º–µ—Ç–æ–¥—ã fetch/parse/run)
- config.py (–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
- requirements.txt (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
–ù–∞–π–¥–∏: –±–∞–≥–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, security issues
```

## Token Counter

**–¢–µ–∫—É—â–∏–µ —Ä–∞–∑–º–µ—Ä—ã:**
- crawler.py: ~140 —Å—Ç—Ä–æ–∫ (~420 —Ç–æ–∫–µ–Ω–æ–≤)
- config.py: ~18 —Å—Ç—Ä–æ–∫ (~40 —Ç–æ–∫–µ–Ω–æ–≤)
- requirements.txt: ~3 —Å—Ç—Ä–æ–∫–∏ (~10 —Ç–æ–∫–µ–Ω–æ–≤)
- Docker: ~15 —Å—Ç—Ä–æ–∫ (~30 —Ç–æ–∫–µ–Ω–æ–≤)

**TOTAL: ~500 —Ç–æ–∫–µ–Ω–æ–≤** (—Å–∞–º—ã–π –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)

## Optimization Tips

‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã (–Ω–µ –≥—Ä—É–∑ –≤–µ—Å—å —Ä–µ–ø–æ)
‚úÖ –ü–µ—Ä–µ–¥–∞–≤–∞–π —Ç–æ–ª—å–∫–æ –¥–µ–ª—å—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ñ–∞–π–ª –∫–∞–∫ reference –≤–º–µ—Å—Ç–æ –∫–æ–¥–∞
‚úÖ –î–ª—è –±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π - –Ω–æ–≤–∞—è –≤–µ—Ç–∫–∞

## Repository Structure

```
web-crawler/
‚îú‚îÄ‚îÄ crawler.py           (async crawler)
‚îú‚îÄ‚îÄ config.py            (config)
‚îú‚îÄ‚îÄ requirements.txt      (deps: 3 only)
‚îú‚îÄ‚îÄ docker-compose.yml    (optional)
‚îú‚îÄ‚îÄ Dockerfile            (optional)
‚îú‚îÄ‚îÄ .env.example          (config template)
‚îú‚îÄ‚îÄ .gitignore            (security)
‚îú‚îÄ‚îÄ LICENSE               (MIT)
‚îú‚îÄ‚îÄ README.md             (short)
‚îú‚îÄ‚îÄ AI_INTEGRATION.md     (this file)
‚îî‚îÄ‚îÄ .github/workflows/
    ‚îî‚îÄ‚îÄ tests.yml         (CI/CD)
```
