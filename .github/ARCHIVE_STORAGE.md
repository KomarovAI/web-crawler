# ğŸ“¦ WHERE ARCHIVES ARE STORED

**ĞĞ Ğ¥Ğ˜Ğ’Ğ« Ğ¥Ğ ĞĞĞ¯Ğ¢Ğ¡Ğ¯ Ğ’ ĞĞ”ĞĞĞœ ĞœĞ•Ğ¡Ğ¢Ğ•: GitHub Actions Artifacts**

---

## ğŸŸ¢ GitHub Actions Artifacts (Ğ“Ğ›ĞĞ’ĞĞĞ•)

### ĞŸÑƒÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°:
```
https://github.com/KomarovAI/web-crawler/actions
â†“
Select workflow run
â†“
"Artifacts" tab
â†“
Download crawl-results
```

### Ğ§Ñ‚Ğ¾ Ñ‚Ğ°Ğ¼:
```
âœ… archive.db          SQLite database (Ğ²ĞµÑÑŒ ĞºÑ€Ğ°ÑƒĞ»)
âœ… archive.warc.gz     ISO 28500:2017 format
âœ… archive.wacz        Browser-playable
âœ… CRAWL_REPORT.md     Report Ğ¾ ĞºÑ€Ğ°ÑƒĞ»Ğµ
```

### Ğ¢Ğ¸Ğ¿ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ:
```
â±ï¸  Ğ’Ñ€ĞµĞ¼Ñ Ğ¶Ğ¸Ğ·Ğ½Ğ¸: 90 Ğ´Ğ½ĞµĞ¹ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼ĞµĞ½ÑÑ‚ÑŒ)
ğŸ“¦ Ğ Ğ°Ğ·Ğ¼ĞµÑ€: ~125 MB per archive
ğŸ’¾ Ğ›Ğ¸Ğ¼Ğ¸Ñ‚: ~400 GB per repo
ğŸ”’ Ğ’Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ: Private to repo (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ¸Ğ´ÑÑ‚)
```

---

## ğŸ”— ĞšĞĞš Ğ¡ĞšĞĞ§ĞĞ¢Ğ¬

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: GitHub Web UI (Easiest)

```
1. https://github.com/KomarovAI/web-crawler
2. Actions tab (Ğ²ĞµÑ€Ñ…Ğ½ÑÑ Ğ¿Ğ°Ğ½ĞµĞ»ÑŒ)
3. Select latest workflow run
4. "Artifacts" section
5. Click "crawl-results"
6. Download zip
7. Extract *.db / *.warc.gz / *.wacz
```

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: GitHub CLI

```bash
# List runs
gh run list --repo KomarovAI/web-crawler

# Download artifacts from latest run
gh run list --repo KomarovAI/web-crawler --limit 1 --json databaseId -q | head -1 | xargs -I {} gh run download {} --repo KomarovAI/web-crawler
```

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 3: GitHub API

```bash
# Get latest artifacts
curl https://api.github.com/repos/KomarovAI/web-crawler/actions/artifacts \
  -H "Authorization: token $GITHUB_TOKEN" | jq '.artifacts[] | {name, url: .archive_download_url}'
```

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 4: From AI Agent (Python)

```python
import requests
import os
import zipfile
from io import BytesIO

# Get latest artifacts
response = requests.get(
    'https://api.github.com/repos/KomarovAI/web-crawler/actions/artifacts',
    headers={'Authorization': f'token {os.environ["GITHUB_TOKEN"]}'}
)

artifacts = response.json()['artifacts']

if artifacts:
    artifact = artifacts[0]  # Latest
    
    # Download
    zip_url = artifact['archive_download_url']
    zip_data = requests.get(
        zip_url,
        headers={'Authorization': f'token {os.environ["GITHUB_TOKEN"]}'}
    ).content
    
    # Extract
    with zipfile.ZipFile(BytesIO(zip_data)) as z:
        z.extractall('.')
    
    # Query
    import sqlite3
    conn = sqlite3.connect('archive.db')
    c = conn.cursor()
    c.execute('SELECT url, title FROM pages LIMIT 10')
    pages = c.fetchall()
    print(pages)
```

---

## ğŸ“Š STORAGE HIERARCHY

```
GitHub Server
    â†“
    â””â”€ Actions Tab
        â””â”€ Workflow Runs
            â””â”€ Artifacts (90 days)
                â”œâ”€ crawl-results
                â”‚   â”œâ”€ *.db
                â”‚   â”œâ”€ *.warc.gz
                â”‚   â”œâ”€ *.wacz
                â”‚   â””â”€ CRAWL_REPORT.md
                â””â”€ batch-summary (for batch crawls)
```

---

## ğŸ”„ Ğ–Ğ˜Ğ—ĞĞ•ĞĞĞ«Ğ™ Ğ¦Ğ˜ĞšĞ› ĞĞ Ğ¥Ğ˜Ğ’Ğ

```
1. Trigger workflow
   â†“
2. GitHub runner starts
   â”œâ”€ /home/runner/work/web-crawler/web-crawler/ (temporary)
   â”œâ”€ Runs smart_archiver_v2.py
   â”œâ”€ Creates *.db / *.warc.gz / *.wacz
   â””â”€ (~125 MB temp storage)
   â†“
3. Upload to Artifacts
   â””â”€ actions/upload-artifact@v4
   â†“
4. Stored in GitHub Actions Artifacts
   â”œâ”€ 90 days retention (default)
   â”œâ”€ Visible in Actions tab
   â”œâ”€ Downloadable via UI/CLI/API
   â””â”€ ~400 GB total limit per repo
   â†“
5. After 90 days
   â””â”€ Automatically deleted
```

---

## ğŸ“‹ WORKFLOW CONFIGURATION

### crawl-website.yml
```yaml
# Single site crawl
# Saves to: Artifacts (crawl-results)
# Retention: 90 days
# Manual + scheduled triggers

steps:
  - run: python3 smart_archiver_v2.py ...
  - uses: actions/upload-artifact@v4
    with:
      name: crawl-results
      path: |
        *.db
        *.warc.gz
        *.wacz
        CRAWL_REPORT.md
      retention-days: 90
```

### batch-crawl.yml
```yaml
# Multiple sites (parallel, max 3)
# Each site: separate artifact
# Saves to: Artifacts (batch-results-DOMAIN)
# Retention: 90 days
```

---

## âš¡ QUICK ACCESS

### Fastest way to get latest archive:

```bash
# Using GitHub CLI (simplest)
gh run list --repo KomarovAI/web-crawler --limit 1 --json databaseId -q | head -1 | xargs -I {} gh run download {} --repo KomarovAI/web-crawler --pattern "*.db"

# Or: GitHub web UI
# 1. Actions tab
# 2. Latest run
# 3. Artifacts â†’ crawl-results â†’ Download
```

---

## ğŸ” VIEW IN GITHUB WEB

```
https://github.com/KomarovAI/web-crawler/actions
                                         â†‘ Click here
                                         
â†’ Workflows
â†’ Latest run
â†’ Artifacts
â†’ crawl-results (ZIP)
â†’ Extract & query with sqlite3
```

---

## ğŸ“Š RETENTION POLICY

```
â±ï¸  Default: 90 days
ğŸ”§ Can change in workflow:
   retention-days: 7    (shorter)
   retention-days: 365  (longer)
   retention-days: 1    (delete immediately)
```

---

## âœ… SUMMARY

| Where | Size | Time | Access | Cost |
|-------|------|------|--------|------|
| **Artifacts** | 125 MB | 90 days | Web/CLI/API | FREE |
| Runner disk | 125 MB | Minutes | Local only | Temp |
| Releases | - | FOREVER | - | Old way |

**â†’ YOU USE: Artifacts (not Releases!)**

---

## ğŸš€ FOR AI AGENTS

```python
# AI can download latest archive:
import os
import requests
import zipfile
from io import BytesIO
import sqlite3

token = os.environ['GITHUB_TOKEN']

# Get latest artifacts
resp = requests.get(
    'https://api.github.com/repos/KomarovAI/web-crawler/actions/artifacts',
    headers={'Authorization': f'token {token}'}
)
artifacts = resp.json()['artifacts']

if artifacts:
    # Download latest
    url = artifacts[0]['archive_download_url']
    zip_data = requests.get(url, headers={'Authorization': f'token {token}'}).content
    
    # Extract & query
    with zipfile.ZipFile(BytesIO(zip_data)) as z:
        z.extractall()
    
    conn = sqlite3.connect('archive.db')
    c = conn.cursor()
    c.execute('SELECT * FROM pages')
    # AI analysis here
```

---

**STATUS:** ğŸŸ¢ All archives in GitHub Actions Artifacts  
**RETENTION:** 90 days (configurable)  
**ACCESS:** Web UI, CLI, API, or programmatically  
**COST:** FREE
