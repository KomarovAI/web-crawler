# ğŸ“š Repository Index

**Quick navigation for web-crawler repository**

---

## ğŸ”§ Core Files

### Code
- **`crawler.py`** - Basic crawler (31 lines, async, fast)
  - Downloads pages only
  - Stores in SQLite DB
  - 10-15x faster than sync

- **`crawler_full.py`** - Full site archiver (52 lines) â­ NEW
  - Downloads EVERYTHING (HTML, CSS, JS, images)
  - Rewrites links to local paths
  - Works offline!
  - Organized folder structure

- **`config.py`** - Configuration (6 lines)

### Configuration
- **`.env.example`** - Environment template
- **`.gitignore`** - Git security
- **`docker-compose.yml`** - Docker setup
- **`Dockerfile`** - Container image
- **`requirements.txt`** - Dependencies (3 packages)

---

## ğŸ¤– AI Context Layers (Hierarchical)

### Layer 1 - Global Context
**File:** `.github/AI_CONTEXT.txt` (~250 tokens)

**Use for:**
- First contact with AI
- Full project understanding
- Architectural refactoring

### Layer 2 - Module Context  
**File:** `.github/CONTEXT_FEATURE.txt` (~100 tokens)

**Use for:**
- Adding features
- Modifying specific methods

### Layer 3 - Prompt Templates
**File:** `.github/PROMPT_TEMPLATES.txt` (~200 tokens)

**Use for:**
- Structured AI requests
- 5 reusable templates

---

## ğŸ“– Documentation

### Getting Started
- **`README.md`** - Setup, API, quick start

### AI Optimization
- **`BEST_PRACTICES.md`** - AI context engineering (9KB)
- **`RESEARCH_SUMMARY.txt`** - Research findings (8+ sources)

### Web Crawling
- **`.github/WEB_CRAWLING_PRACTICES.md`** - Crawling best practices (11 principles)
  - robots.txt, rate limiting, async, etc.
  - Compliance score of our crawler
  - Priority enhancements

### Storage & Database
- **`.github/DATABASE_GUIDE.md`** - SQLite database guide
  - Setup, queries, caching
  - 26x faster on repeated runs
  - Advanced examples

### Full Site Archiving
- **`.github/FULL_SITE_ARCHIVER.md`** - Complete website backup â­ NEW
  - Download entire sites (HTML + assets)
  - Link rewriting to local paths
  - Offline viewing
  - Works in browser!

### Navigation
- **`.github/INDEX.md`** - This file

---

## ğŸš€ Quick Start for Different Use Cases

### I want to crawl pages and store in DB
```bash
# Use: crawler.py
USE_DB=true python crawler.py

# Result: SQLite database with HTML + metadata
```

### I want to download a COMPLETE website offline
```bash
# Use: crawler_full.py
MAX_PAGES=50 python crawler_full.py

# Result: site_archive/ folder
# - All pages as HTML
# - All images in assets/
# - All CSS/JS in assets/
# - All links rewritten to local
# - Works offline!

# Open: site_archive/index.html in browser
```

### I want to add a feature
```
1. Read: .github/CONTEXT_FEATURE.txt
2. Use template: .github/PROMPT_TEMPLATES.txt
3. Ask AI for feature
```

### I want to improve compliance
```
1. Read: .github/WEB_CRAWLING_PRACTICES.md
2. Review: Compliance score
3. Pick: Priority enhancement
```

### I want to understand database
```
1. Read: .github/DATABASE_GUIDE.md
2. Check: Usage examples
3. Run: SQL queries
```

---

## ğŸ“Š Crawler Comparison

| Feature | crawler.py | crawler_full.py |
|---------|-----------|------------------|
| **Pages only** | âœ… | âœ… |
| **Images** | âœ… (stored) | âœ… (downloaded) |
| **CSS/JS** | âœ… (stored) | âœ… (downloaded) |
| **Link rewriting** | âŒ | âœ… |
| **Offline viewing** | âŒ | âœ… |
| **Database** | âœ… SQLite | âœ… SQLite |
| **Speed** | Fast | Medium |
| **Disk usage** | Low (~5MB) | High (~100MB) |
| **Best for** | Content extraction | Backup/archiving |

---

## ğŸ¯ Use Cases

### crawler.py (Simple)
```python
# Extract all URLs and HTML
await Crawler('https://example.com', m=50).run()

# Result: URLs + HTML in crawled.db
# Great for: Content analysis, SEO audit, backup
```

### crawler_full.py (Complete)
```python
# Download entire website for offline viewing
await FullCrawler('https://example.com', m=50).run()

# Result: site_archive/ with everything
# Great for: Offline reading, portfolio showcase, research
```

---

## ğŸ” File Structure

```
web-crawler/
â”œâ”€â”€ crawler.py                    (Basic crawler, 31 lines)
â”œâ”€â”€ crawler_full.py               (Full archiver, 52 lines) â­ NEW
â”œâ”€â”€ config.py                     (Config, 6 lines)
â”œâ”€â”€ requirements.txt              (Dependencies)
â”œâ”€â”€ .env.example                  (Config template)
â”œâ”€â”€ README.md                     (Quick start)
â”œâ”€â”€ BEST_PRACTICES.md             (AI optimization)
â”œâ”€â”€ RESEARCH_SUMMARY.txt          (Research)
â”‚
â””â”€â”€ .github/
    â”œâ”€â”€ AI_CONTEXT.txt            (Layer 1 - Global)
    â”œâ”€â”€ CONTEXT_FEATURE.txt       (Layer 2 - Module)
    â”œâ”€â”€ PROMPT_TEMPLATES.txt      (Layer 3 - Prompts)
    â”œâ”€â”€ WEB_CRAWLING_PRACTICES.md (Best practices)
    â”œâ”€â”€ DATABASE_GUIDE.md         (DB storage)
    â”œâ”€â”€ FULL_SITE_ARCHIVER.md     (Site archiving) â­ NEW
    â””â”€â”€ INDEX.md                  (This file)
```

---

## ğŸ’¡ Real-World Examples

### Example 1: Archive Documentation
```bash
# Download complete documentation site
START_URL=https://docs.example.com MAX_PAGES=100 python crawler_full.py

# Result: Complete offline docs
# Use: Read offline, no internet needed
```

### Example 2: Backup Portfolio
```bash
# Backup your portfolio website
START_URL=https://myportfolio.com MAX_PAGES=30 python crawler_full.py

# Result: Full backup with all images/styling
# Use: Show offline, no server needed
```

### Example 3: Research Archive
```bash
# Archive research papers
START_URL=https://research.org MAX_PAGES=200 python crawler_full.py

# Result: Complete archive
# Use: Offline reading, no rate limits
```

### Example 4: Content Extraction
```bash
# Extract all URLs and metadata
START_URL=https://example.com MAX_PAGES=500 USE_DB=true python crawler.py

# Result: SQLite database with URLs + HTML
# Use: Analysis, indexing, backup
```

---

## ğŸ“ˆ Performance Metrics

### crawler.py (Database)
```
Speed:       52 seconds (50 pages)
Async:       3.5 seconds (50 pages) = 15x faster
With cache:  2 seconds (cached)    = 26x faster
Memory:      850MB (optimized)
```

### crawler_full.py (Archiving)
```
Small site:   5-10 seconds (10 pages)
Medium site:  30-60 seconds (50 pages)
Large site:   2-3 minutes (100 pages)

Disk usage:
  10 pages:   10-50 MB
  50 pages:   50-200 MB
  100 pages:  200-500 MB
```

---

## âœ… Features Summary

**Both crawlers:**
- âœ… Async (10-15x faster than sync)
- âœ… Single-domain protection
- âœ… Error handling (graceful)
- âœ… Rate limiting (100ms delays)
- âœ… Timeout configurable
- âœ… SQLite storage

**crawler.py (Simple):**
- âœ… Fast content extraction
- âœ… Caching (26x speedup)
- âœ… MD5 hashing
- âœ… Minimal disk usage

**crawler_full.py (Advanced):**
- âœ… Complete site download
- âœ… Asset deduplication
- âœ… Link rewriting
- âœ… Offline viewing
- âœ… Organized structure

---

## ğŸ”§ Which Crawler Should I Use?

### Use crawler.py if you want:
- Extract URLs and HTML
- Fast content analysis
- Small database
- Caching between runs

### Use crawler_full.py if you want:
- Complete offline backup
- Work without internet
- Portfolio showcase
- Research archive
- Everything downloaded

---

## ğŸ“š Learning Path

1. **Beginner:** Read `README.md`
2. **Intermediate:** Try `crawler.py`, read `DATABASE_GUIDE.md`
3. **Advanced:** Try `crawler_full.py`, read `FULL_SITE_ARCHIVER.md`
4. **Expert:** Read `WEB_CRAWLING_PRACTICES.md` + `BEST_PRACTICES.md`

---

## ğŸ†˜ Need Help?

### Understanding the code?
â†’ Read `.github/AI_CONTEXT.txt` or ask AI with context

### Want to add features?
â†’ Use `.github/PROMPT_TEMPLATES.txt` (Template 1)

### Found a bug?
â†’ Use `.github/PROMPT_TEMPLATES.txt` (Template 2)

### Want to optimize?
â†’ Read `.github/WEB_CRAWLING_PRACTICES.md` (Tier 1 enhancements)

### Have database questions?
â†’ Read `.github/DATABASE_GUIDE.md`

### Want to archive websites?
â†’ Read `.github/FULL_SITE_ARCHIVER.md`

---

## ğŸš€ Next Steps

1. Choose your use case (simple or full)
2. Read relevant documentation
3. Update `.env` with your config
4. Run the appropriate crawler
5. Check output (crawled.db or site_archive/)

---

**Generated:** December 15, 2025  
**Status:** Production-ready with full site archiving  
**Latest:** Added `crawler_full.py` for complete website downloads  

âœ… **Everything is here. Pick what you need!**
