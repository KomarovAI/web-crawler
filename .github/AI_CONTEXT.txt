WEB CRAWLER - ULTRA MINIMAL AI CONTEXT (~250 tokens)

TECH: Python 3.11, aiohttp, beautifulsoup4, python-dotenv (3 deps only)

FILES:
crawler.py: class Crawler(url, max_p, t)
  - async fetch(s, url) -> str|None
  - async parse(h, b) -> list[str]
  - async run() -> {total, urls}
  - _ok(url) -> bool (validate: domain, visited, max_p)

config.py: START_URL, MAX_PAGES, TIMEOUT env vars
requirements.txt: aiohttp==3.9.1, beautifulsoup4==4.12.2, python-dotenv==1.0.0

ARCHITECTURE:
Queue-based BFS -> fetch HTML -> parse links -> validate -> enqueue
- TCPConnector(limit=5) for concurrent requests
- asyncio.sleep(0.1) rate limit
- visited set (O1 lookup)
- domain filtering

ENV VARS:
START_URL=https://example.com
MAX_PAGES=50
TIMEOUT=10

FLOW:
1. Crawler(url, max_pages) init
2. run() loop:
   - dequeue url
   - fetch with aiohttp (timeout, ssl=False)
   - parse with BeautifulSoup (find all <a>)
   - filter: _ok(url) checks domain+visited+max
   - enqueue new links
   - sleep 100ms
3. return {total, urls}

ERROR HANDLING:
- fetch() returns None on any error (no exceptions)
- parse() safe BeautifulSoup parsing
- _ok() filters bad URLs before fetch
- No bubble-up exceptions

PERF:
- Concurrent: 5 requests
- Rate: 0.1s between fetches
- Memory: O(n) visited set
- Network bound

COMMON MODS:
1. Headers: session.get(url, headers={...})
2. Parallelism: TCPConnector(limit=10)
3. Rate: asyncio.sleep(0.05)
4. Save: add results export
5. Extract: expand parse() return

AVOID:
- NO requests (use aiohttp)
- NO Selenium (too heavy)
- NO sync calls (all async)
- NO creds in code
- NO 20+ deps

TOKEN COUNT: ~250 for this, ~200 for code = ~450 total
USAGE: Copy this entire file when asking AI for changes
