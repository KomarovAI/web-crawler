name: üï∑Ô∏è Crawl

on:
  workflow_dispatch:
    inputs:
      sites_json:
        description: 'JSON array: [{"url":"https://example.com","max_pages":50}]'
        required: true
        default: '[{"url":"https://example.com","max_pages":50}]'

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        site: ${{ fromJson(github.event.inputs.sites_json) }}
      max-parallel: 3
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install deps
        run: pip install -r requirements.txt
      
      - name: Setup site
        id: site
        run: |
          python << 'EOF'
          import json
          site = json.loads('${{ toJson(matrix.site) }}')
          url = site['url']
          max_pages = site.get('max_pages', 50)
          domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_')
          db_file = f"{domain}.db"
          
          print(f"URL: {url}")
          print(f"Max pages: {max_pages}")
          print(f"DB: {db_file}")
          
          with open('.env', 'w') as f:
            f.write(f"START_URL={url}\n")
            f.write(f"MAX_PAGES={max_pages}\n")
            f.write(f"DB_FILE={db_file}\n")
          EOF
          
          domain=$(python3 -c "import json; site=json.loads('${{ toJson(matrix.site) }}'); url=site['url']; print(url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_'))")
          echo "domain=${domain}" >> $GITHUB_OUTPUT
          echo "db_file=${domain}.db" >> $GITHUB_OUTPUT
      
      - name: Crawl
        run: python crawler.py
      
      - name: Upload DB
        uses: actions/upload-artifact@v4
        with:
          name: dbs
          path: ${{ steps.site.outputs.db_file }}
          retention-days: 90
