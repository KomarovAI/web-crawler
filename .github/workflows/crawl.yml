name: üï∑Ô∏è Professional Web Archive

on:
  workflow_dispatch:
    inputs:
      sites_json:
        description: 'JSON: [{"url":"https://example.com","max_pages":50}]'
        required: true
        default: '[{"url":"https://callmedley.com","max_pages":500}]'

jobs:
  archive:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      matrix:
        site: ${{ fromJson(github.event.inputs.sites_json) }}
      max-parallel: 5
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      # ‚ö° pip cache
      - name: ‚ö° Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: '**/requirements.txt'
      
      - name: Install deps
        run: pip install -r requirements.txt
      
      - name: Setup site
        id: site
        env:
          SITE_JSON: ${{ toJson(matrix.site) }}
        run: |
          python3 << 'PYTHON'
          import json
          import os
          
          site = json.loads(os.environ['SITE_JSON'])
          url = site['url']
          max_pages = site.get('max_pages', 50)
          domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_')
          
          with open('.env', 'w') as f:
            f.write(f"START_URL={url}\n")
            f.write(f"MAX_PAGES={max_pages}\n")
          
          with open('domain.txt', 'w') as f:
            f.write(domain)
          PYTHON
          
          domain=$(cat domain.txt)
          echo "domain=${domain}" >> $GITHUB_OUTPUT
          echo "archive_dir=archive_${domain}" >> $GITHUB_OUTPUT
      
      # üï∑Ô∏è Run professional archiver
      - name: üï∑Ô∏è Run Professional Archiver (WARC ISO 28500:2017)
        id: archiver
        continue-on-error: true  # Continue even if archiver encounters errors
        run: |
          python3 smart_archiver_v3.py "${{ matrix.site.url }}" ${{ matrix.site.max_pages }}
          echo "status=$?" >> $GITHUB_OUTPUT
        timeout-minutes: 120
      
      # üìä Show archive structure
      - name: üìÅ Show Archive Structure
        if: always()
        run: |
          domain="${{ steps.site.outputs.domain }}"
          archive_dir="archive_${domain}"
          
          if [ -d "${archive_dir}" ]; then
            echo "‚úÖ Archive directory created: ${archive_dir}"
            echo "======================== ARCHIVE STRUCTURE ========================"
            tree -L 2 "${archive_dir}" 2>/dev/null || find "${archive_dir}" -type f | head -30
            echo ""
            echo "Directory sizes:"
            du -sh "${archive_dir}"/* 2>/dev/null || du -sh "${archive_dir}"
            
            # Check for error log
            if [ -f "${archive_dir}/errors.json" ]; then
              echo ""
              echo "‚ö†Ô∏è  Errors encountered:"
              head -20 "${archive_dir}/errors.json"
            fi
          else
            echo "‚ùå Archive directory NOT created: ${archive_dir}"
            echo "Checking current directory:"
            ls -la | grep archive
          fi
      
      # üì¶ Upload full archive (NOT compressed as zip)
      - name: üì¶ Upload Archive as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: archive-${{ steps.site.outputs.domain }}
          path: archive_${{ steps.site.outputs.domain }}/
          retention-days: 90
          compression-level: 0
          if-no-files-found: warn  # Warn instead of fail if directory doesn't exist

  verify:
    needs: archive
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: üìÅ Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloads
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: ‚úÖ VERIFY ARCHIVE STRUCTURE
        run: |
          python3 << 'VERIFY_EOF'
          import json
          from pathlib import Path
          import sys
          
          print("\n" + "="*80)
          print("‚úÖ ARCHIVE VERIFICATION")
          print("="*80)
          
          downloads_dir = Path('downloads')
          archive_dirs = [d for d in downloads_dir.glob('archive-*') if d.is_dir()]
          
          if not archive_dirs:
              print("\n‚ö†Ô∏è  No archive directories found!")
              print("This can happen if:")
              print("  1. The target server returned HTTP 500")
              print("  2. Network connection failed")
              print("  3. Timeout occurred")
              print("\nCheck the archiver logs above for details.")
              sys.exit(0)  # Exit gracefully
          
          for archive_dir in sorted(archive_dirs):
              domain = archive_dir.name.replace('archive-', '')
              print(f"\nüìÇ Checking: {domain}")
              print("-" * 80)
              
              # Check structure
              required_dirs = ['pages', 'assets', 'warc']
              assets_subdirs = ['images', 'styles', 'scripts', 'fonts']
              
              for required_dir in required_dirs:
                  path = archive_dir / required_dir
                  if path.exists():
                      count = len(list(path.rglob('*')))
                      size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / 1024 / 1024
                      print(f"  ‚úÖ {required_dir:10} - {count:4} items, {size:8.2f} MB")
                  else:
                      print(f"  ‚ö†Ô∏è  {required_dir:10} - (empty)")
              
              # Check assets subdirs
              assets_dir = archive_dir / 'assets'
              if assets_dir.exists():
                  print(f"\n  Assets subdirectories:")
                  for subdir in assets_subdirs:
                      path = assets_dir / subdir
                      if path.exists():
                          count = len(list(path.glob('*')))
                          size = sum(f.stat().st_size for f in path.glob('*') if f.is_file()) / 1024 / 1024
                          print(f"    ‚Ä¢ {subdir:12} - {count:4} files, {size:8.2f} MB")
              
              # Check database
              db_file = archive_dir / f'{domain}.db'
              if db_file.exists():
                  db_size = db_file.stat().st_size / 1024 / 1024
                  print(f"\n  ‚úÖ Database:       {db_file.name} ({db_size:.2f} MB)")
              else:
                  print(f"\n  ‚ö†Ô∏è  Database: (not created)")
              
              # Check metadata
              metadata_file = archive_dir / 'metadata.json'
              if metadata_file.exists():
                  with open(metadata_file) as f:
                      metadata = json.load(f)
                  print(f"\n  ‚úÖ Metadata: {metadata_file.name}")
                  print(f"     Pages: {metadata.get('pages_archived', 'N/A')}")
                  print(f"     Assets: {metadata.get('assets_archived', 'N/A')}")
                  print(f"     Total Size: {metadata.get('total_assets_size_mb', 'N/A')} MB")
                  print(f"     Errors: {metadata.get('total_errors', 0)}")
              else:
                  print(f"\n  ‚ö†Ô∏è  Metadata: (not created)")
              
              # Check error log
              error_file = archive_dir / 'errors.json'
              if error_file.exists():
                  with open(error_file) as f:
                      errors = json.load(f)
                  print(f"\n  ‚ö†Ô∏è  Error Log: {len(errors)} errors recorded")
                  # Show first 3 errors
                  for i, error in enumerate(errors[:3]):
                      print(f"     {i+1}. [{error['type']}] {error['url'][:50]}")
                      print(f"        ‚Üí {error['message'][:60]}")
              
              # Total size
              total_size = sum(f.stat().st_size for f in archive_dir.rglob('*') if f.is_file()) / 1024 / 1024
              print(f"\n  üìä TOTAL ARCHIVE SIZE: {total_size:.2f} MB")
              print(f"\n  ‚úÖ Archive structure is READY FOR DEPLOYMENT")
          
          print("\n" + "="*80)
          print("‚úÖ VERIFICATION COMPLETE")
          print("="*80 + "\n")
          VERIFY_EOF
