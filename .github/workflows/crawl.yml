name: üï∑Ô∏è Professional Web Archive

on:
  workflow_dispatch:
    inputs:
      sites_json:
        description: 'JSON: [{"url":"https://example.com","max_pages":50}]'
        required: true
        default: '[{"url":"https://callmedley.com","max_pages":500}]'

jobs:
  archive:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      matrix:
        site: ${{ fromJson(github.event.inputs.sites_json) }}
      max-parallel: 5
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      # ‚ö° pip cache
      - name: ‚ö° Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: '**/requirements.txt'
      
      - name: Install deps
        run: pip install -r requirements.txt
      
      - name: Setup site
        id: site
        env:
          SITE_JSON: ${{ toJson(matrix.site) }}
        run: |
          python3 << 'PYTHON'
          import json
          import os
          
          site = json.loads(os.environ['SITE_JSON'])
          url = site['url']
          max_pages = site.get('max_pages', 50)
          domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_')
          
          with open('.env', 'w') as f:
            f.write(f"START_URL={url}\n")
            f.write(f"MAX_PAGES={max_pages}\n")
          
          with open('domain.txt', 'w') as f:
            f.write(domain)
          PYTHON
          
          domain=$(cat domain.txt)
          echo "domain=${domain}" >> $GITHUB_OUTPUT
          echo "archive_dir=archive_${domain}" >> $GITHUB_OUTPUT
      
      # üï∑Ô∏è Run professional archiver
      - name: üï∑Ô∏è Run Professional Archiver (WARC ISO 28500:2017)
        run: |
          python3 smart_archiver_v3.py "${{ matrix.site.url }}" ${{ matrix.site.max_pages }}
        timeout-minutes: 120
      
      # üìä Show archive structure
      - name: üìÅ Show Archive Structure
        run: |
          domain="${{ steps.site.outputs.domain }}"
          archive_dir="archive_${domain}"
          
          echo "======================== ARCHIVE STRUCTURE ========================"
          tree -L 2 "${archive_dir}" || find "${archive_dir}" -type f | head -20
          echo ""
          echo "Directory sizes:"
          du -sh "${archive_dir}"/*
      
      # üì¶ Upload full archive (NOT compressed as zip)
      - name: üì¶ Upload Archive as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: archive-${{ steps.site.outputs.domain }}
          path: archive_${{ steps.site.outputs.domain }}/
          retention-days: 90
          compression-level: 0

  verify:
    needs: archive
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: üìÅ Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloads
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: ‚úÖ VERIFY ARCHIVE STRUCTURE
        run: |
          python3 << 'VERIFY_EOF'
          import json
          from pathlib import Path
          import sys
          
          print("\n" + "="*80)
          print("‚úÖ ARCHIVE VERIFICATION")
          print("="*80)
          
          downloads_dir = Path('downloads')
          archive_dirs = [d for d in downloads_dir.glob('archive-*') if d.is_dir()]
          
          if not archive_dirs:
              print("\n‚ùå No archive directories found!")
              sys.exit(1)
          
          for archive_dir in sorted(archive_dirs):
              domain = archive_dir.name.replace('archive-', '')
              print(f"\nüìÇ Checking: {domain}")
              print("-" * 80)
              
              # Check structure
              required_dirs = ['pages', 'assets', 'warc']
              assets_subdirs = ['images', 'styles', 'scripts', 'fonts']
              
              for required_dir in required_dirs:
                  path = archive_dir / required_dir
                  if path.exists():
                      count = len(list(path.rglob('*')))
                      size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / 1024 / 1024
                      print(f"  ‚úÖ {required_dir:10} - {count:4} items, {size:8.2f} MB")
                  else:
                      print(f"  ‚ùå {required_dir:10} - MISSING")
              
              # Check assets subdirs
              assets_dir = archive_dir / 'assets'
              if assets_dir.exists():
                  print(f"\n  Assets subdirectories:")
                  for subdir in assets_subdirs:
                      path = assets_dir / subdir
                      if path.exists():
                          count = len(list(path.glob('*')))
                          size = sum(f.stat().st_size for f in path.glob('*') if f.is_file()) / 1024 / 1024
                          print(f"    ‚Ä¢ {subdir:12} - {count:4} files, {size:8.2f} MB")
              
              # Check database
              db_file = archive_dir / f'{domain}.db'
              if db_file.exists():
                  db_size = db_file.stat().st_size / 1024 / 1024
                  print(f"\n  ‚úÖ Database:       {db_file.name} ({db_size:.2f} MB)")
              else:
                  print(f"\n  ‚ùå Database: MISSING")
              
              # Check metadata
              metadata_file = archive_dir / 'metadata.json'
              if metadata_file.exists():
                  with open(metadata_file) as f:
                      metadata = json.load(f)
                  print(f"\n  ‚úÖ Metadata: {metadata_file.name}")
                  print(f"     Pages: {metadata.get('pages_archived', 'N/A')}")
                  print(f"     Assets: {metadata.get('assets_archived', 'N/A')}")
                  print(f"     Total Size: {metadata.get('total_assets_size_mb', 'N/A')} MB")
              else:
                  print(f"\n  ‚ö†Ô∏è  Metadata: MISSING")
              
              # Total size
              total_size = sum(f.stat().st_size for f in archive_dir.rglob('*') if f.is_file()) / 1024 / 1024
              print(f"\n  üìä TOTAL ARCHIVE SIZE: {total_size:.2f} MB")
              print(f"\n  ‚úÖ Archive structure is VALID and READY FOR DEPLOYMENT")
          
          print("\n" + "="*80)
          print("‚úÖ VERIFICATION COMPLETE")
          print("="*80 + "\n")
          VERIFY_EOF
