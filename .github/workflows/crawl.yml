name: üï∑Ô∏è Crawl

on:
  workflow_dispatch:
    inputs:
      sites_json:
        description: 'JSON: [{"url":"https://example.com","max_pages":50}]'
        required: true
        default: '[{"url":"https://example.com","max_pages":50}]'

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        site: ${{ fromJson(github.event.inputs.sites_json) }}
      max-parallel: 3
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install deps
        run: pip install -r requirements.txt
      
      - name: Setup site
        id: site
        run: |
          python3 << 'PYTHON'
          import json
          site = json.loads('${{ toJson(matrix.site) }}')
          url = site['url']
          max_pages = site.get('max_pages', 50)
          domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_')
          
          with open('.env', 'w') as f:
            f.write(f"START_URL={url}\n")
            f.write(f"MAX_PAGES={max_pages}\n")
            f.write(f"DB_FILE={domain}.db\n")
          
          with open('domain.txt', 'w') as f:
            f.write(domain)
          PYTHON
          
          domain=$(cat domain.txt)
          echo "domain=${domain}" >> $GITHUB_OUTPUT
          echo "db_file=${domain}.db" >> $GITHUB_OUTPUT
      
      - name: Run crawler
        run: python crawler.py
      
      - name: Verify DB
        run: |
          if [ -f "${{ steps.site.outputs.db_file }}" ]; then
            ls -lh "${{ steps.site.outputs.db_file }}"
            echo "‚úÖ DB created successfully"
          fi
      
      - name: Store in artifact (no-zip)
        uses: actions/upload-artifact@v4
        with:
          name: db-${{ steps.site.outputs.domain }}
          path: ${{ steps.site.outputs.db_file }}
          retention-days: 90
          if-no-files-found: error
          compression-level: 0
