# @description Downloads and archives website content with Selenium/Python crawler
# @requires Python 3.11, requirements.txt, smart_archiver_v4.py
# @outputs Artifact: site-archive-RUN_ID (90 days retention)
# @uses GitHub Actions, Python, Selenium, PostgreSQL (optional)

name: Download Site (Artifact Only)

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Start URL (http/https)'
        required: true
        type: string
      max_pages:
        description: 'Max pages to crawl (1-5000)'
        required: false
        type: string
        default: '100'
      use_selenium:
        description: 'Use Selenium for Cloudflare (true/false)'
        required: false
        type: string
        default: 'true'
      ignore_robots:
        description: 'Ignore robots.txt (true/false)'
        required: false
        type: string
        default: 'false'
      target_repo:
        description: 'Target repository for upload (owner/repo)'
        required: false
        type: string
        default: ''
      auto_deploy:
        description: 'Auto deploy after download (true/false)'
        required: false
        type: string
        default: 'false'
      resumeUrl:
        description: 'n8n webhook callback URL (for dispatchAndWait)'
        required: false
        type: string
        default: ''

permissions:
  contents: read
  actions: read

jobs:
  download-site:
    name: Download Site
    runs-on: ubuntu-24.04
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Setup Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: requirements.txt
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Validate and normalize inputs
        id: normalize
        run: |
          # Extract and normalize inputs
          URL="${{ github.event.inputs.url }}"
          MAX_PAGES="${{ github.event.inputs.max_pages }}"
          USE_SELENIUM="${{ github.event.inputs.use_selenium }}"
          IGNORE_ROBOTS="${{ github.event.inputs.ignore_robots }}"
          AUTO_DEPLOY="${{ github.event.inputs.auto_deploy }}"
          
          # Apply defaults if empty
          URL=${URL:-"https://callmedley.com"}
          MAX_PAGES=${MAX_PAGES:-"100"}
          USE_SELENIUM=${USE_SELENIUM:-"true"}
          IGNORE_ROBOTS=${IGNORE_ROBOTS:-"false"}
          AUTO_DEPLOY=${AUTO_DEPLOY:-"false"}
          
          # Validate URL format
          if [[ ! "$URL" =~ ^https?:// ]]; then
            echo "‚ùå Error: URL must start with http:// or https://"
            exit 1
          fi
          
          # Validate max_pages is numeric and in range
          MAX_PAGES_INT=${MAX_PAGES//[^0-9]/}
          if ! [[ "$MAX_PAGES_INT" =~ ^[0-9]+$ ]] || (( MAX_PAGES_INT < 1 || MAX_PAGES_INT > 5000 )); then
            echo "‚ùå Error: max_pages must be 1-5000, got: $MAX_PAGES"
            exit 1
          fi
          
          # Normalize boolean strings to lowercase
          USE_SELENIUM=$(echo "$USE_SELENIUM" | tr '[:upper:]' '[:lower:]')
          IGNORE_ROBOTS=$(echo "$IGNORE_ROBOTS" | tr '[:upper:]' '[:lower:]')
          AUTO_DEPLOY=$(echo "$AUTO_DEPLOY" | tr '[:upper:]' '[:lower:]')
          
          # Extract domain from URL
          DOMAIN=$(echo "$URL" | sed -E 's|^https?://||' | sed -E 's|/.*||' | cut -d'?' -f1)
          
          # Output normalized values
          echo "url=$URL" >> $GITHUB_OUTPUT
          echo "max_pages=$MAX_PAGES_INT" >> $GITHUB_OUTPUT
          echo "use_selenium=$USE_SELENIUM" >> $GITHUB_OUTPUT
          echo "ignore_robots=$IGNORE_ROBOTS" >> $GITHUB_OUTPUT
          echo "auto_deploy=$AUTO_DEPLOY" >> $GITHUB_OUTPUT
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Inputs validated and normalized:"
          echo "  - URL: $URL"
          echo "  - Max Pages: $MAX_PAGES_INT"
          echo "  - Domain: $DOMAIN"
          echo "  - Use Selenium: $USE_SELENIUM"
          echo "  - Ignore robots.txt: $IGNORE_ROBOTS"
      
      - name: Download site with smart_archiver_v4.py
        run: |
          python3 << 'PYTHON'
          url = '${{ steps.normalize.outputs.url }}'
          max_pages = '${{ steps.normalize.outputs.max_pages }}'
          domain = '${{ steps.normalize.outputs.domain }}'
          
          with open('.env', 'w') as f:
              f.write(f"STARTURL={url}\n")
              f.write(f"MAXPAGES={max_pages}\n")
          
          with open('domain.txt', 'w') as f:
              f.write(domain)
          
          print(f"‚úÖ Configuration written:")
          print(f"   URL: {url}")
          print(f"   Max Pages: {max_pages}")
          print(f"   Domain: {domain}")
          PYTHON
          
          source .env
          python3 smart_archiver_v4.py "$STARTURL" "$MAXPAGES"
        env:
          USE_SELENIUM: ${{ steps.normalize.outputs.use_selenium }}
          IGNORE_ROBOTS: ${{ steps.normalize.outputs.ignore_robots }}
      
      - name: Verify archive
        run: |
          ARCHIVE_DIR=$(find . -maxdepth 1 -name "archive_*" -type d | head -1)
          if [ -z "$ARCHIVE_DIR" ]; then
            echo "‚ùå Archive not found!"
            exit 1
          fi
          
          echo "‚úÖ Archive found: $ARCHIVE_DIR"
          echo "üìÉ Archive contents:"
          du -sh "$ARCHIVE_DIR"
          find "$ARCHIVE_DIR" -type f | head -20
      
      - name: Upload archive artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: site-archive-${{ github.run_id }}
          path: archive_*/
          retention-days: 90
          compression-level: 6
          if-no-files-found: warn
      
      - name: Create summary
        if: always()
        run: |
          echo "## ‚úÖ Site Download Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **URL** | ${{ steps.normalize.outputs.url }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Max Pages** | ${{ steps.normalize.outputs.max_pages }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Domain** | ${{ steps.normalize.outputs.domain }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Selenium** | ${{ steps.normalize.outputs.use_selenium }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Ignore robots.txt** | ${{ steps.normalize.outputs.ignore_robots }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Auto Deploy** | ${{ steps.normalize.outputs.auto_deploy }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifact Name**: site-archive-${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Retention**: 90 days" >> $GITHUB_STEP_SUMMARY
      
      - name: Notify n8n on completion
        if: always() && github.event.inputs.resumeUrl != ''
        run: |
          RESUME_URL="${{ github.event.inputs.resumeUrl }}"
          JOB_STATUS="${{ job.status }}"
          RUN_ID="${{ github.run_id }}"
          DOMAIN="${{ steps.normalize.outputs.domain }}"
          MAX_PAGES="${{ steps.normalize.outputs.max_pages }}"
          TIMESTAMP="$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          ARTIFACT_NAME="site-archive-${{ github.run_id }}"
          
          if [ -n "$RESUME_URL" ]; then
            echo "üì° Sending callback to n8n..."
            
            # Use printf to properly escape JSON
            PAYLOAD=$(printf '{
              "status": "%s",
              "conclusion": "%s",
              "run_id": %s,
              "workflow": "download-site",
              "domain": "%s",
              "max_pages": "%s",
              "artifact_name": "%s",
              "timestamp": "%s"
            }' "$JOB_STATUS" "$JOB_STATUS" "$RUN_ID" "$DOMAIN" "$MAX_PAGES" "$ARTIFACT_NAME" "$TIMESTAMP")
            
            echo "Payload: $PAYLOAD"
            
            curl -k \
              -X POST "$RESUME_URL" \
              -H "Content-Type: application/json" \
              -d "$PAYLOAD" \
              --retry 3 \
              --retry-delay 5 \
              --connect-timeout 10 \
              --max-time 30 \
              -w "\n‚úÖ Callback sent (HTTP %{http_code})\n" || echo "‚ö†Ô∏è  Callback failed (non-critical)"
          fi
