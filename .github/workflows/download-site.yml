name: Download Website Archive

concurrency:
  group: download-${{ github.event.inputs.url }}-${{ github.event.inputs.depth_level }}
  cancel-in-progress: true

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Website URL to download (e.g., https://example.com)'
        required: false
        default: 'https://callmedley.com'
        type: string
      depth_level:
        description: 'Crawl depth: 1=homepage, 2=+children (default), 3=+grandchildren, 4=very deep'
        required: false
        default: '2'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
          - '4'
      output_dir:
        description: 'Output directory name (alphanumeric, dash, underscore only)'
        required: false
        default: 'site_archive'
        type: string
      parallel_jobs:
        description: 'Number of parallel download jobs (1-10)'
        required: false
        default: '10'
        type: choice
        options:
          - '1'
          - '5'
          - '10'
      resumeUrl:
        description: 'N8N webhook URL for callback (auto-filled by N8N)'
        required: false
        type: string

permissions: {}

jobs:
  extract-urls:
    name: Extract URLs from sitemap
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      urls_matrix: ${{ steps.generate-matrix.outputs.matrix }}
      has_sitemap: ${{ steps.check-sitemap.outputs.exists }}
      base_domain: ${{ steps.validate.outputs.domain }}
    
    steps:
      - name: Validate inputs
        id: validate
        env:
          INPUT_URL: ${{ github.event.inputs.url || 'https://callmedley.com' }}
          INPUT_DEPTH: ${{ github.event.inputs.depth_level || '2' }}
          INPUT_OUTPUT: ${{ github.event.inputs.output_dir || 'site_archive' }}
        run: |
          set -e

          if [[ ! "$INPUT_URL" =~ ^https?:// ]]; then
            echo "‚ùå URL must start with http or https"
            exit 1
          fi

          DOMAIN=$(echo "$INPUT_URL" | sed 's|https://||g' | sed 's|http://||g' | cut -d'/' -f1)
          SANITIZED_DIR=$(echo "$INPUT_OUTPUT" | tr -cd '[:alnum:]_-')
          SANITIZED_DIR=${SANITIZED_DIR:-site_archive}

          echo "output_dir=$SANITIZED_DIR" >> $GITHUB_OUTPUT
          echo "url=$INPUT_URL" >> $GITHUB_OUTPUT
          echo "depth_level=$INPUT_DEPTH" >> $GITHUB_OUTPUT
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT
          
          echo "üìã Validated: $DOMAIN | depth=$INPUT_DEPTH"

      - name: Check for sitemap.xml
        id: check-sitemap
        continue-on-error: true
        env:
          URL: ${{ steps.validate.outputs.url }}
        run: |
          set -e
          
          for SITEMAP in "sitemap.xml" "sitemap_index.xml" "sitemap-index.xml"; do
            SITEMAP_URL="${URL}/${SITEMAP}"
            echo "üîç Checking $SITEMAP_URL"
            
            if curl -sf --max-time 10 -I "$SITEMAP_URL" | grep -q "200 OK"; then
              echo "‚úÖ Found sitemap: $SITEMAP_URL"
              echo "exists=true" >> $GITHUB_OUTPUT
              echo "sitemap_url=$SITEMAP_URL" >> $GITHUB_OUTPUT
              exit 0
            fi
          done
          
          echo "‚ö†Ô∏è No sitemap found, will use depth-based crawl"
          echo "exists=false" >> $GITHUB_OUTPUT

      - name: Extract URLs from sitemap
        if: steps.check-sitemap.outputs.exists == 'true'
        id: extract-sitemap
        env:
          SITEMAP_URL: ${{ steps.check-sitemap.outputs.sitemap_url }}
          PARALLEL_JOBS: ${{ github.event.inputs.parallel_jobs || 10 }}
        run: |
          set -e
          
          echo "üì• Downloading sitemap from $SITEMAP_URL"
          wget -q -O sitemap.xml "$SITEMAP_URL"
          
          grep -oP '(?<=<loc>)[^<]+' sitemap.xml | head -1000 > urls.txt || true
          
          if grep -q "<sitemap>" sitemap.xml; then
            echo "üì¶ Sitemap index detected, extracting nested sitemaps"
            grep -oP '(?<=<loc>)[^<]+' sitemap.xml | while read NESTED_URL; do
              wget -q -O - "$NESTED_URL" | grep -oP '(?<=<loc>)[^<]+' >> urls.txt || true
            done
          fi
          
          TOTAL_URLS=$(wc -l < urls.txt)
          echo "‚úÖ Extracted $TOTAL_URLS URLs from sitemap"
          echo "total_urls=$TOTAL_URLS" >> $GITHUB_OUTPUT

      - name: Generate fallback URLs (no sitemap)
        if: steps.check-sitemap.outputs.exists != 'true'
        env:
          URL: ${{ steps.validate.outputs.url }}
        run: |
          echo "$URL" > urls.txt
          echo "‚ö†Ô∏è Using depth-based crawl strategy"

      - name: Generate matrix for parallel download
        id: generate-matrix
        env:
          PARALLEL_JOBS: ${{ github.event.inputs.parallel_jobs || 10 }}
        run: |
          set -e
          
          TOTAL_URLS=$(wc -l < urls.txt)
          CHUNK_SIZE=$(( (TOTAL_URLS + PARALLEL_JOBS - 1) / PARALLEL_JOBS ))
          
          echo "üìä Splitting $TOTAL_URLS URLs into $PARALLEL_JOBS chunks (chunk_size=$CHUNK_SIZE)"
          
          split -l $CHUNK_SIZE urls.txt chunk_ -da 2
          
          CHUNKS=$(ls chunk_* | jq -R -s -c 'split("\n")[:-1]')
          echo "matrix={\"chunk\":$CHUNKS}" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Generated matrix with $(echo $CHUNKS | jq '. | length') chunks"

      - name: Upload URL chunks
        uses: actions/upload-artifact@v4
        with:
          name: url-chunks-${{ github.run_id }}
          path: chunk_*
          retention-days: 1
          if-no-files-found: error

  parallel-download:
    name: Download chunk ${{ matrix.chunk }}
    needs: extract-urls
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: needs.extract-urls.outputs.urls_matrix != ''
    
    strategy:
      fail-fast: false
      max-parallel: 10
      matrix: ${{ fromJson(needs.extract-urls.outputs.urls_matrix) }}
    
    steps:
      - name: Download URL chunks
        uses: actions/download-artifact@v4
        with:
          name: url-chunks-${{ github.run_id }}

      - name: Parallel download with GNU Parallel
        id: download
        continue-on-error: true
        env:
          CHUNK: ${{ matrix.chunk }}
          DEPTH: ${{ github.event.inputs.depth_level || 2 }}
          OUTPUT_DIR: site_archive_${{ matrix.chunk }}
          HAS_SITEMAP: ${{ needs.extract-urls.outputs.has_sitemap }}
        run: |
          set -e
          
          sudo apt-get update -qq
          sudo apt-get install -y wget parallel
          
          mkdir -p "$OUTPUT_DIR"
          
          echo "üöÄ Starting parallel download for chunk: $CHUNK"
          START_TIME=$(date +%s)
          
          if [[ "$HAS_SITEMAP" == "true" ]]; then
            echo "üì• Downloading URLs from sitemap chunk"
            cat "$CHUNK" | parallel -j 5 --timeout 60 \
              "wget -q -P '$OUTPUT_DIR' \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --timeout=30 \
                --tries=2 \
                --user-agent='Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)' \
                {} || true"
          else
            echo "üîÑ Recursive download (depth=$DEPTH)"
            while read URL; do
              wget --recursive \
                --level="$DEPTH" \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --no-parent \
                --directory-prefix="$OUTPUT_DIR" \
                --timeout=30 \
                --tries=2 \
                --wait=1 \
                --random-wait \
                --user-agent="Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)" \
                --reject-regex='\?.*' \
                "$URL" 2>&1 | head -100 || true
            done < "$CHUNK"
          fi
          
          END_TIME=$(date +%s)
          ELAPSED=$((END_TIME - START_TIME))
          
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo "0")
          
          echo "elapsed=$ELAPSED" >> $GITHUB_OUTPUT
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "dir_size=$DIR_SIZE" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Chunk $CHUNK completed in ${ELAPSED}s: $FILE_COUNT files, $DIR_SIZE"

      - name: Validate chunk
        id: validate
        continue-on-error: true
        env:
          OUTPUT_DIR: site_archive_${{ matrix.chunk }}
        run: |
          set -e
          
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          TOTAL_SIZE=$(du -sb "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo 0)
          
          # Validation thresholds
          MIN_FILES=1
          MIN_SIZE=1024  # 1KB minimum
          
          if [ "$FILE_COUNT" -lt "$MIN_FILES" ]; then
            echo "‚ùå Chunk validation FAILED: only $FILE_COUNT files"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          if [ "$TOTAL_SIZE" -lt "$MIN_SIZE" ]; then
            echo "‚ùå Chunk validation FAILED: only $TOTAL_SIZE bytes"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "‚úÖ Chunk validation PASSED: $FILE_COUNT files, $TOTAL_SIZE bytes"
          echo "valid=true" >> $GITHUB_OUTPUT

      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chunk-${{ matrix.chunk }}-${{ github.run_id }}
          path: site_archive_${{ matrix.chunk }}
          retention-days: 1
          if-no-files-found: warn
          compression-level: 0

      - name: Mark chunk status
        if: always()
        env:
          CHUNK: ${{ matrix.chunk }}
          VALID: ${{ steps.validate.outputs.valid }}
          DOWNLOAD_RESULT: ${{ steps.download.outcome }}
        run: |
          mkdir -p chunk_status
          
          if [[ "$VALID" == "true" && "$DOWNLOAD_RESULT" == "success" ]]; then
            echo "success" > "chunk_status/$CHUNK.status"
            echo "‚úÖ Chunk $CHUNK marked as SUCCESS"
          else
            echo "failed" > "chunk_status/$CHUNK.status"
            echo "‚ùå Chunk $CHUNK marked as FAILED"
          fi

      - name: Upload chunk status
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: status-${{ matrix.chunk }}-${{ github.run_id }}
          path: chunk_status/
          retention-days: 1

  detect-failed-chunks:
    name: Detect failed chunks
    needs: [extract-urls, parallel-download]
    runs-on: ubuntu-latest
    if: always()
    timeout-minutes: 5
    outputs:
      failed_chunks: ${{ steps.analyze.outputs.failed_chunks }}
      has_failures: ${{ steps.analyze.outputs.has_failures }}
      retry_matrix: ${{ steps.analyze.outputs.retry_matrix }}
    
    steps:
      - name: Download all status artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: status-*-${{ github.run_id }}
          path: statuses/
          merge-multiple: true

      - name: Analyze chunk statuses
        id: analyze
        run: |
          set -e
          
          echo "üîç Analyzing chunk statuses..."
          
          FAILED_CHUNKS="[]"
          FAILED_COUNT=0
          
          if [ -d "statuses" ]; then
            for STATUS_FILE in statuses/*.status; do
              if [ -f "$STATUS_FILE" ]; then
                CHUNK=$(basename "$STATUS_FILE" .status)
                STATUS=$(cat "$STATUS_FILE")
                
                echo "Chunk $CHUNK: $STATUS"
                
                if [ "$STATUS" = "failed" ]; then
                  FAILED_CHUNKS=$(echo "$FAILED_CHUNKS" | jq --arg chunk "$CHUNK" '. + [$chunk]')
                  FAILED_COUNT=$((FAILED_COUNT + 1))
                fi
              fi
            done
          fi
          
          echo "failed_chunks=$FAILED_CHUNKS" >> $GITHUB_OUTPUT
          
          if [ "$FAILED_COUNT" -gt 0 ]; then
            echo "has_failures=true" >> $GITHUB_OUTPUT
            echo "retry_matrix={\"chunk\":$FAILED_CHUNKS}" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Found $FAILED_COUNT failed chunks: $FAILED_CHUNKS"
          else
            echo "has_failures=false" >> $GITHUB_OUTPUT
            echo "retry_matrix={}" >> $GITHUB_OUTPUT
            echo "‚úÖ All chunks completed successfully"
          fi

  retry-failed-chunks:
    name: Retry chunk ${{ matrix.chunk }}
    needs: [extract-urls, detect-failed-chunks]
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: needs.detect-failed-chunks.outputs.has_failures == 'true'
    
    strategy:
      fail-fast: false
      max-parallel: 10
      matrix: ${{ fromJson(needs.detect-failed-chunks.outputs.retry_matrix) }}
    
    steps:
      - name: Download URL chunks
        uses: actions/download-artifact@v4
        with:
          name: url-chunks-${{ github.run_id }}

      - name: Retry download with exponential backoff
        id: retry
        env:
          CHUNK: ${{ matrix.chunk }}
          DEPTH: ${{ github.event.inputs.depth_level || 2 }}
          OUTPUT_DIR: site_archive_${{ matrix.chunk }}_retry
          HAS_SITEMAP: ${{ needs.extract-urls.outputs.has_sitemap }}
        run: |
          set -e
          
          sudo apt-get update -qq
          sudo apt-get install -y wget parallel
          
          mkdir -p "$OUTPUT_DIR"
          
          echo "üîÑ RETRY: Starting download for failed chunk: $CHUNK"
          
          # Exponential backoff: wait before retry
          WAIT_TIME=$((RANDOM % 10 + 5))  # 5-15 seconds jitter
          echo "‚è≥ Waiting ${WAIT_TIME}s before retry (jitter to avoid thundering herd)..."
          sleep $WAIT_TIME
          
          START_TIME=$(date +%s)
          
          if [[ "$HAS_SITEMAP" == "true" ]]; then
            echo "üì• Retry: Downloading URLs from sitemap chunk"
            cat "$CHUNK" | parallel -j 3 --timeout 90 --retries 2 \
              "wget -q -P '$OUTPUT_DIR' \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --timeout=45 \
                --tries=3 \
                --waitretry=5 \
                --user-agent='Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)' \
                {} || true"
          else
            echo "üîÑ Retry: Recursive download (depth=$DEPTH)"
            while read URL; do
              wget --recursive \
                --level="$DEPTH" \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --no-parent \
                --directory-prefix="$OUTPUT_DIR" \
                --timeout=45 \
                --tries=3 \
                --waitretry=5 \
                --wait=2 \
                --random-wait \
                --user-agent="Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)" \
                --reject-regex='\?.*' \
                "$URL" 2>&1 | head -100 || true
            done < "$CHUNK"
          fi
          
          END_TIME=$(date +%s)
          ELAPSED=$((END_TIME - START_TIME))
          
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo "0")
          
          echo "‚úÖ Retry completed for $CHUNK in ${ELAPSED}s: $FILE_COUNT files, $DIR_SIZE"

      - name: Upload retry chunk artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chunk-${{ matrix.chunk }}-retry-${{ github.run_id }}
          path: site_archive_${{ matrix.chunk }}_retry
          retention-days: 1
          if-no-files-found: warn
          compression-level: 0

  merge-results:
    name: Merge all chunks
    needs: [extract-urls, parallel-download, detect-failed-chunks, retry-failed-chunks]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: always()
    
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: chunk-*-${{ github.run_id }}
          path: chunks/
          merge-multiple: false

      - name: Merge chunks
        id: merge
        env:
          OUTPUT_DIR: ${{ github.event.inputs.output_dir || 'site_archive' }}
        run: |
          set -e
          
          mkdir -p "$OUTPUT_DIR"
          
          echo "üîÑ Merging chunks into $OUTPUT_DIR"
          
          MERGED_COUNT=0
          for CHUNK_DIR in chunks/*/; do
            if [ -d "$CHUNK_DIR" ]; then
              CHUNK_NAME=$(basename "$CHUNK_DIR")
              FILE_COUNT=$(find "$CHUNK_DIR" -type f 2>/dev/null | wc -l || echo 0)
              
              if [ "$FILE_COUNT" -gt 0 ]; then
                echo "üì¶ Merging $CHUNK_NAME ($FILE_COUNT files)"
                cp -r "$CHUNK_DIR"/* "$OUTPUT_DIR"/ 2>/dev/null || true
                MERGED_COUNT=$((MERGED_COUNT + 1))
              else
                echo "‚ö†Ô∏è Skipping empty chunk: $CHUNK_NAME"
              fi
            fi
          done
          
          echo "merged_chunks=$MERGED_COUNT" >> $GITHUB_OUTPUT
          
          # Verify merged archive
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          HTML_COUNT=$(find "$OUTPUT_DIR" -type f \( -name "*.html" -o -name "*.htm" \) 2>/dev/null | wc -l || echo 0)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo "0")
          TOTAL_SIZE=$(du -sb "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo 0)
          
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "html_count=$HTML_COUNT" >> $GITHUB_OUTPUT
          echo "dir_size=$DIR_SIZE" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Merged $MERGED_COUNT chunks: $FILE_COUNT files ($HTML_COUNT HTML), $DIR_SIZE"
          
          if [ "$HTML_COUNT" -lt 1 ]; then
            echo "‚ö†Ô∏è Warning: No HTML files in merged archive"
          fi
          
          if [ "$TOTAL_SIZE" -lt 10240 ]; then
            echo "‚ö†Ô∏è Warning: Archive too small: $TOTAL_SIZE bytes"
          fi

      - name: Upload final artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}
          path: ${{ github.event.inputs.output_dir || 'site_archive' }}
          retention-days: 30
          if-no-files-found: warn
          compression-level: 0

      - name: Job summary
        if: always()
        env:
          URL: ${{ github.event.inputs.url || 'https://callmedley.com' }}
          DEPTH: ${{ github.event.inputs.depth_level || 2 }}
          PARALLEL: ${{ github.event.inputs.parallel_jobs || 10 }}
          FILE_COUNT: ${{ steps.merge.outputs.file_count }}
          HTML_COUNT: ${{ steps.merge.outputs.html_count }}
          DIR_SIZE: ${{ steps.merge.outputs.dir_size }}
          MERGED_CHUNKS: ${{ steps.merge.outputs.merged_chunks }}
          FAILED_CHUNKS: ${{ needs.detect-failed-chunks.outputs.failed_chunks }}
          HAS_FAILURES: ${{ needs.detect-failed-chunks.outputs.has_failures }}
        run: |
          echo "## üìä Parallel Download Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: $URL" >> $GITHUB_STEP_SUMMARY
          echo "- Depth: $DEPTH" >> $GITHUB_STEP_SUMMARY
          echo "- Parallel Jobs: $PARALLEL runners" >> $GITHUB_STEP_SUMMARY
          echo "- Sitemap: ${{ needs.extract-urls.outputs.has_sitemap }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "$HAS_FAILURES" == "true" ]]; then
            RETRY_COUNT=$(echo "$FAILED_CHUNKS" | jq 'length')
            echo "**Retry Status:**" >> $GITHUB_STEP_SUMMARY
            echo "- Failed chunks retried: $RETRY_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- Failed chunk IDs: $FAILED_CHUNKS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ -n "$FILE_COUNT" ]] && [[ "$FILE_COUNT" -gt 0 ]]; then
            echo "**Status: ‚úÖ SUCCESS**" >> $GITHUB_STEP_SUMMARY
            echo "- Files: $FILE_COUNT ($HTML_COUNT HTML)" >> $GITHUB_STEP_SUMMARY
            echo "- Size: $DIR_SIZE" >> $GITHUB_STEP_SUMMARY
            echo "- Merged chunks: $MERGED_CHUNKS" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Status: ‚ùå FAILED** (no files downloaded)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifact:** \`${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY

      - name: Send callback to N8N
        if: always() && github.event.inputs.resumeUrl
        continue-on-error: true
        env:
          FILES: ${{ steps.merge.outputs.file_count || 0 }}
          SIZE: ${{ steps.merge.outputs.dir_size || 'unknown' }}
          RETRY_COUNT: ${{ needs.detect-failed-chunks.outputs.has_failures == 'true' && '1' || '0' }}
        run: |
          STATUS="success"
          if [ -z "$FILES" ] || [ "$FILES" = "0" ]; then
            STATUS="failed"
          fi
          
          for i in {1..3}; do
            if curl -s --max-time 10 -X POST "${{ github.event.inputs.resumeUrl }}" \
              -H "Content-Type: application/json" \
              -d "{
                \"status\": \"$STATUS\",
                \"files\": $FILES,
                \"size\": \"$SIZE\",
                \"url\": \"${{ github.event.inputs.url }}\",
                \"depth\": ${{ github.event.inputs.depth_level || 2 }},
                \"parallel_jobs\": ${{ github.event.inputs.parallel_jobs || 10 }},
                \"retried_chunks\": $RETRY_COUNT,
                \"failed_chunks\": ${{ needs.detect-failed-chunks.outputs.failed_chunks }},
                \"run_id\": \"${{ github.run_id }}\",
                \"artifact_name\": \"${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}\"
              }"; then
              echo "‚úÖ Callback sent (attempt $i)"
              break
            else
              echo "‚ö†Ô∏è Callback attempt $i failed"
              [ $i -lt 3 ] && sleep 2
            fi
          done
