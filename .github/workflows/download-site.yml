name: Download Website Archive

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Website URL to download (e.g., https://example.com)'
        required: false
        default: 'https://callmedley.com'
        type: string
      depth_level:
        description: 'Crawl depth: 1=homepage, 2=+children (default), 3=+grandchildren, 4=very deep'
        required: false
        default: '2'
        type: string
      output_dir:
        description: 'Output directory name (no special chars)'
        required: false
        default: 'site_archive'
        type: string
      resumeUrl:
        description: 'N8N webhook URL for callback (auto-filled by N8N)'
        required: false
        type: string

jobs:
  download:
    name: Download Website Archive
    runs-on: ubuntu-24.04
    permissions:
      contents: read
      actions: read

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget
          pip install requests beautifulsoup4 selenium

      - name: Validate inputs
        id: validate
        env:
          INPUT_URL: ${{ github.event.inputs.url || 'https://callmedley.com' }}
          INPUT_DEPTH: ${{ github.event.inputs.depth_level || '2' }}
          INPUT_OUTPUT: ${{ github.event.inputs.output_dir || 'site_archive' }}
        run: |
          # Validate URL
          if [[ ! "$INPUT_URL" =~ ^https?:// ]]; then
            echo "❌ URL must start with http:// or https://"
            exit 1
          fi

          # Validate depth_level
          if ! [[ "$INPUT_DEPTH" =~ ^[0-9]+$ ]] || (( INPUT_DEPTH < 1 )) || (( INPUT_DEPTH > 4 )); then
            echo "❌ depth_level must be 1-4"
            exit 1
          fi

          # Get level description
          case $INPUT_DEPTH in
            1) LEVEL_DESC="homepage only" ;;
            2) LEVEL_DESC="+children" ;;
            3) LEVEL_DESC="+2 levels" ;;
            4) LEVEL_DESC="very deep" ;;
          esac

          # Sanitize directory
          SANITIZED_DIR=$(echo "$INPUT_OUTPUT" | tr -cd '[:alnum:]_-')
          SANITIZED_DIR=${SANITIZED_DIR:-site_archive}

          echo "output_dir=$SANITIZED_DIR" >> $GITHUB_OUTPUT
          echo "url=$INPUT_URL" >> $GITHUB_OUTPUT
          echo "depth_level=$INPUT_DEPTH" >> $GITHUB_OUTPUT
          echo "level_description=$LEVEL_DESC" >> $GITHUB_OUTPUT
          echo "✅ OK: $INPUT_URL (depth=$INPUT_DEPTH)"

      - name: Download site with wget
        id: download
        env:
          URL: ${{ steps.validate.outputs.url }}
          OUTPUT_DIR: ${{ steps.validate.outputs.output_dir }}
          DEPTH: ${{ steps.validate.outputs.depth_level }}
        run: |
          mkdir -p "$OUTPUT_DIR"
          
          # Use --reject-regex to skip URLs with query parameters BEFORE downloading
          # This is more efficient than downloading and then deleting files
          # Pattern: \?.*  matches URLs containing ? followed by any characters
          wget --recursive --level="$DEPTH" \
            --convert-links \
            --page-requisites \
            --adjust-extension \
            --no-parent \
            --directory-prefix="$OUTPUT_DIR" \
            --timeout=30 \
            --tries=3 \
            --user-agent="Mozilla/5.0" \
            --reject-regex='\?.*' \
            "$URL" 2>&1 | tee wget_output.log
          
          # Capture exit code (wget returns 0 for success, 1 for partial success)
          WGET_STATUS=$?
          echo "wget_status=$WGET_STATUS" >> $GITHUB_OUTPUT
          echo "✅ Download completed (status: $WGET_STATUS)"

      - name: Verify archive
        id: verify
        env:
          OUTPUT_DIR: ${{ steps.validate.outputs.output_dir }}
        run: |
          if [ ! -d "$OUTPUT_DIR" ] || [ -z "$(ls -A "$OUTPUT_DIR" 2>/dev/null)" ]; then
            echo "❌ Archive directory is empty or does not exist"
            exit 1
          fi

          FILE_COUNT=$(find "$OUTPUT_DIR" -type f | wc -l)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" | cut -f1)
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "dir_size=$DIR_SIZE" >> $GITHUB_OUTPUT
          echo "upload_path=$OUTPUT_DIR" >> $GITHUB_OUTPUT
          echo "✅ Verified: $FILE_COUNT files, $DIR_SIZE"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ${{ steps.validate.outputs.output_dir }}-${{ github.run_id }}
          path: ${{ steps.verify.outputs.upload_path }}
          retention-days: 30
          if-no-files-found: warn

      - name: Send callback to N8N
        if: always() && github.event.inputs.resumeUrl
        run: |
          curl -s -X POST "${{ github.event.inputs.resumeUrl }}" \
            -H "Content-Type: application/json" \
            -d "{
              \"status\": \"${{ job.status }}\",
              \"files\": ${{ steps.verify.outputs.file_count || 0 }},
              \"size\": \"${{ steps.verify.outputs.dir_size }}\",
              \"url\": \"${{ steps.validate.outputs.url }}\",
              \"depth\": ${{ steps.validate.outputs.depth_level }},
              \"run_id\": \"${{ github.run_id }}\"
            }" || true
