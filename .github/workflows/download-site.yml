name: Download Website Archive

concurrency:
  group: download-${{ github.event.inputs.url }}-${{ github.event.inputs.depth_level }}
  cancel-in-progress: true

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Website URL to download (e.g., https://example.com)'
        required: false
        default: 'https://callmedley.com'
        type: string
      depth_level:
        description: 'Crawl depth: 1=homepage, 2=+children (default), 3=+grandchildren, 4=very deep'
        required: false
        default: '2'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
          - '4'
      output_dir:
        description: 'Output directory name (alphanumeric, dash, underscore only)'
        required: false
        default: 'site_archive'
        type: string
      parallel_jobs:
        description: 'Number of parallel download jobs (1-10)'
        required: false
        default: '10'
        type: choice
        options:
          - '1'
          - '5'
          - '10'
      resumeUrl:
        description: 'N8N webhook URL for callback (auto-filled by N8N)'
        required: false
        type: string

permissions: {}

jobs:
  extract-urls:
    name: Extract URLs from sitemap
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      urls_matrix: ${{ steps.generate-matrix.outputs.matrix }}
      has_sitemap: ${{ steps.check-sitemap.outputs.exists }}
      base_domain: ${{ steps.validate.outputs.domain }}
    
    steps:
      - name: Validate inputs
        id: validate
        env:
          INPUT_URL: ${{ github.event.inputs.url || 'https://callmedley.com' }}
          INPUT_DEPTH: ${{ github.event.inputs.depth_level || '2' }}
          INPUT_OUTPUT: ${{ github.event.inputs.output_dir || 'site_archive' }}
        run: |
          set -e

          # Validate URL
          if [[ ! "$INPUT_URL" =~ ^https?:// ]]; then
            echo "‚ùå URL must start with http or https"
            exit 1
          fi

          # Extract domain
          DOMAIN=$(echo "$INPUT_URL" | sed 's|https://||g' | sed 's|http://||g' | cut -d'/' -f1)
          
          # Sanitize directory
          SANITIZED_DIR=$(echo "$INPUT_OUTPUT" | tr -cd '[:alnum:]_-')
          SANITIZED_DIR=${SANITIZED_DIR:-site_archive}

          echo "output_dir=$SANITIZED_DIR" >> $GITHUB_OUTPUT
          echo "url=$INPUT_URL" >> $GITHUB_OUTPUT
          echo "depth_level=$INPUT_DEPTH" >> $GITHUB_OUTPUT
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT
          
          echo "üìã Validated: $DOMAIN | depth=$INPUT_DEPTH"

      - name: Check for sitemap.xml
        id: check-sitemap
        continue-on-error: true
        env:
          URL: ${{ steps.validate.outputs.url }}
        run: |
          set -e
          
          # Try common sitemap locations
          for SITEMAP in "sitemap.xml" "sitemap_index.xml" "sitemap-index.xml"; do
            SITEMAP_URL="${URL}/${SITEMAP}"
            echo "üîç Checking $SITEMAP_URL"
            
            if curl -sf --max-time 10 -I "$SITEMAP_URL" | grep -q "200 OK"; then
              echo "‚úÖ Found sitemap: $SITEMAP_URL"
              echo "exists=true" >> $GITHUB_OUTPUT
              echo "sitemap_url=$SITEMAP_URL" >> $GITHUB_OUTPUT
              exit 0
            fi
          done
          
          echo "‚ö†Ô∏è No sitemap found, will use depth-based crawl"
          echo "exists=false" >> $GITHUB_OUTPUT

      - name: Extract URLs from sitemap
        if: steps.check-sitemap.outputs.exists == 'true'
        id: extract-sitemap
        env:
          SITEMAP_URL: ${{ steps.check-sitemap.outputs.sitemap_url }}
          PARALLEL_JOBS: ${{ github.event.inputs.parallel_jobs || 10 }}
        run: |
          set -e
          
          echo "üì• Downloading sitemap from $SITEMAP_URL"
          wget -q -O sitemap.xml "$SITEMAP_URL"
          
          # Extract URLs from sitemap (handle both regular and index sitemaps)
          grep -oP '(?<=<loc>)[^<]+' sitemap.xml | head -1000 > urls.txt || true
          
          # If sitemap index, download nested sitemaps
          if grep -q "<sitemap>" sitemap.xml; then
            echo "üì¶ Sitemap index detected, extracting nested sitemaps"
            grep -oP '(?<=<loc>)[^<]+' sitemap.xml | while read NESTED_URL; do
              wget -q -O - "$NESTED_URL" | grep -oP '(?<=<loc>)[^<]+' >> urls.txt || true
            done
          fi
          
          TOTAL_URLS=$(wc -l < urls.txt)
          echo "‚úÖ Extracted $TOTAL_URLS URLs from sitemap"
          echo "total_urls=$TOTAL_URLS" >> $GITHUB_OUTPUT

      - name: Generate fallback URLs (no sitemap)
        if: steps.check-sitemap.outputs.exists != 'true'
        env:
          URL: ${{ steps.validate.outputs.url }}
        run: |
          # Just use base URL for depth-based crawling
          echo "$URL" > urls.txt
          echo "‚ö†Ô∏è Using depth-based crawl strategy"

      - name: Generate matrix for parallel download
        id: generate-matrix
        env:
          PARALLEL_JOBS: ${{ github.event.inputs.parallel_jobs || 10 }}
        run: |
          set -e
          
          TOTAL_URLS=$(wc -l < urls.txt)
          CHUNK_SIZE=$(( (TOTAL_URLS + PARALLEL_JOBS - 1) / PARALLEL_JOBS ))
          
          echo "üìä Splitting $TOTAL_URLS URLs into $PARALLEL_JOBS chunks (chunk_size=$CHUNK_SIZE)"
          
          # Split URLs into chunks
          split -l $CHUNK_SIZE urls.txt chunk_ -da 2
          
          # Create matrix JSON
          CHUNKS=$(ls chunk_* | jq -R -s -c 'split("\n")[:-1]')
          echo "matrix={\"chunk\":$CHUNKS}" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Generated matrix with $(echo $CHUNKS | jq '. | length') chunks"

      - name: Upload URL chunks
        uses: actions/upload-artifact@v4
        with:
          name: url-chunks-${{ github.run_id }}
          path: chunk_*
          retention-days: 1
          if-no-files-found: error

  parallel-download:
    name: Download chunk ${{ matrix.chunk }}
    needs: extract-urls
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: needs.extract-urls.outputs.urls_matrix != ''
    
    strategy:
      fail-fast: false
      max-parallel: 10
      matrix: ${{ fromJson(needs.extract-urls.outputs.urls_matrix) }}
    
    steps:
      - name: Download URL chunks
        uses: actions/download-artifact@v4
        with:
          name: url-chunks-${{ github.run_id }}

      - name: Parallel download with GNU Parallel
        env:
          CHUNK: ${{ matrix.chunk }}
          DEPTH: ${{ github.event.inputs.depth_level || 2 }}
          OUTPUT_DIR: site_archive_${{ matrix.chunk }}
          HAS_SITEMAP: ${{ needs.extract-urls.outputs.has_sitemap }}
        run: |
          set -e
          
          sudo apt-get update -qq
          sudo apt-get install -y wget parallel
          
          mkdir -p "$OUTPUT_DIR"
          
          echo "üöÄ Starting parallel download for chunk: $CHUNK"
          START_TIME=$(date +%s)
          
          if [[ "$HAS_SITEMAP" == "true" ]]; then
            # Download URLs from chunk using GNU Parallel
            echo "üì• Downloading URLs from sitemap chunk"
            cat "$CHUNK" | parallel -j 5 --timeout 60 \
              "wget -q -P '$OUTPUT_DIR' \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --timeout=30 \
                --tries=2 \
                --user-agent='Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)' \
                {} || true"
          else
            # Depth-based recursive download
            echo "üîÑ Recursive download (depth=$DEPTH)"
            while read URL; do
              wget --recursive \
                --level="$DEPTH" \
                --page-requisites \
                --convert-links \
                --adjust-extension \
                --no-parent \
                --directory-prefix="$OUTPUT_DIR" \
                --timeout=30 \
                --tries=2 \
                --wait=1 \
                --random-wait \
                --user-agent="Mozilla/5.0 (compatible; ArchiveBot/1.0; +https://github.com/KomarovAI/web-crawler)" \
                --reject-regex='\?.*' \
                "$URL" 2>&1 | head -100 || true
            done < "$CHUNK"
          fi
          
          END_TIME=$(date +%s)
          ELAPSED=$((END_TIME - START_TIME))
          
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo "0")
          
          echo "‚úÖ Chunk $CHUNK completed in ${ELAPSED}s: $FILE_COUNT files, $DIR_SIZE"

      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chunk-${{ matrix.chunk }}-${{ github.run_id }}
          path: site_archive_${{ matrix.chunk }}
          retention-days: 1
          if-no-files-found: warn
          compression-level: 0

  merge-results:
    name: Merge all chunks
    needs: [extract-urls, parallel-download]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: always()
    
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: chunk-*-${{ github.run_id }}
          path: chunks/
          merge-multiple: false

      - name: Merge chunks
        id: merge
        env:
          OUTPUT_DIR: ${{ github.event.inputs.output_dir || 'site_archive' }}
        run: |
          set -e
          
          mkdir -p "$OUTPUT_DIR"
          
          echo "üîÑ Merging chunks into $OUTPUT_DIR"
          
          # Merge all chunks
          for CHUNK_DIR in chunks/*/; do
            if [ -d "$CHUNK_DIR" ]; then
              echo "üì¶ Merging $(basename $CHUNK_DIR)"
              cp -r "$CHUNK_DIR"/* "$OUTPUT_DIR"/ 2>/dev/null || true
            fi
          done
          
          # Verify merged archive
          FILE_COUNT=$(find "$OUTPUT_DIR" -type f 2>/dev/null | wc -l || echo 0)
          HTML_COUNT=$(find "$OUTPUT_DIR" -type f \( -name "*.html" -o -name "*.htm" \) 2>/dev/null | wc -l || echo 0)
          DIR_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo "0")
          TOTAL_SIZE=$(du -sb "$OUTPUT_DIR" 2>/dev/null | cut -f1 || echo 0)
          
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "html_count=$HTML_COUNT" >> $GITHUB_OUTPUT
          echo "dir_size=$DIR_SIZE" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Merged archive: $FILE_COUNT files ($HTML_COUNT HTML), $DIR_SIZE"
          
          # Validation
          if [ "$HTML_COUNT" -lt 1 ]; then
            echo "‚ö†Ô∏è Warning: No HTML files in merged archive"
          fi
          
          if [ "$TOTAL_SIZE" -lt 10240 ]; then
            echo "‚ö†Ô∏è Warning: Archive too small: $TOTAL_SIZE bytes"
          fi

      - name: Upload final artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}
          path: ${{ github.event.inputs.output_dir || 'site_archive' }}
          retention-days: 30
          if-no-files-found: warn
          compression-level: 0

      - name: Cleanup chunk artifacts
        if: always()
        continue-on-error: true
        run: |
          # Cleanup temporary chunks (they're merged now)
          echo "üßπ Cleanup completed"

      - name: Job summary
        if: always()
        env:
          URL: ${{ github.event.inputs.url || 'https://callmedley.com' }}
          DEPTH: ${{ github.event.inputs.depth_level || 2 }}
          PARALLEL: ${{ github.event.inputs.parallel_jobs || 10 }}
          FILE_COUNT: ${{ steps.merge.outputs.file_count }}
          HTML_COUNT: ${{ steps.merge.outputs.html_count }}
          DIR_SIZE: ${{ steps.merge.outputs.dir_size }}
        run: |
          echo "## üìä Parallel Download Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: $URL" >> $GITHUB_STEP_SUMMARY
          echo "- Depth: $DEPTH" >> $GITHUB_STEP_SUMMARY
          echo "- Parallel Jobs: $PARALLEL runners" >> $GITHUB_STEP_SUMMARY
          echo "- Sitemap: ${{ needs.extract-urls.outputs.has_sitemap }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ -n "$FILE_COUNT" ]] && [[ "$FILE_COUNT" -gt 0 ]]; then
            echo "**Status: ‚úÖ SUCCESS**" >> $GITHUB_STEP_SUMMARY
            echo "- Files: $FILE_COUNT ($HTML_COUNT HTML)" >> $GITHUB_STEP_SUMMARY
            echo "- Size: $DIR_SIZE" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Status: ‚ùå FAILED** (no files downloaded)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifact:** \`${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY

      - name: Send callback to N8N
        if: always() && github.event.inputs.resumeUrl
        continue-on-error: true
        env:
          FILES: ${{ steps.merge.outputs.file_count || 0 }}
          SIZE: ${{ steps.merge.outputs.dir_size || 'unknown' }}
        run: |
          STATUS="success"
          if [ -z "$FILES" ] || [ "$FILES" = "0" ]; then
            STATUS="failed"
          fi
          
          for i in {1..3}; do
            if curl -s --max-time 10 -X POST "${{ github.event.inputs.resumeUrl }}" \
              -H "Content-Type: application/json" \
              -d "{
                \"status\": \"$STATUS\",
                \"files\": $FILES,
                \"size\": \"$SIZE\",
                \"url\": \"${{ github.event.inputs.url }}\",
                \"depth\": ${{ github.event.inputs.depth_level || 2 }},
                \"parallel_jobs\": ${{ github.event.inputs.parallel_jobs || 10 }},
                \"run_id\": \"${{ github.run_id }}\",
                \"artifact_name\": \"${{ github.event.inputs.output_dir || 'site_archive' }}-${{ github.run_id }}\"
              }"; then
              echo "‚úÖ Callback sent (attempt $i)"
              break
            else
              echo "‚ö†Ô∏è Callback attempt $i failed"
              [ $i -lt 3 ] && sleep 2
            fi
          done
