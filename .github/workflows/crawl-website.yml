name: Crawl Website

on:
  workflow_dispatch:
    inputs:
      website_url:
        description: 'Website URL to crawl'
        required: true
        default: 'https://example.com'
      max_pages:
        description: 'Maximum pages to crawl'
        required: false
        default: '50'
      include_assets:
        description: 'Download images, CSS, JS'
        required: false
        type: boolean
        default: true
  schedule:
    # Run every day at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create .env file
        run: |
          cat > .env << EOF
          START_URL=${{ github.event.inputs.website_url || 'https://example.com' }}
          MAX_PAGES=${{ github.event.inputs.max_pages || '50' }}
          TIMEOUT=15
          USE_DB=true
          DB_FILE=crawled.db
          INCLUDE_ASSETS=${{ github.event.inputs.include_assets || 'true' }}
          EOF
      
      - name: Run crawler
        id: crawl
        run: |
          python crawler.py
          echo "crawl_status=success" >> $GITHUB_OUTPUT
      
      - name: Generate report
        if: success()
        run: |
          python << 'EOF'
          import sqlite3
          from datetime import datetime
          
          conn = sqlite3.connect('crawled.db')
          cursor = conn.cursor()
          
          cursor.execute('SELECT COUNT(*) FROM pages')
          total_pages = cursor.fetchone()[0]
          
          cursor.execute('SELECT SUM(content_length) FROM pages')
          total_size = cursor.fetchone()[0] or 0
          
          cursor.execute('SELECT COUNT(*) FROM assets')
          total_assets = cursor.fetchone()[0]
          
          report = f"""
          # ðŸ“Š Crawl Report
          
          **Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Website:** ${{ github.event.inputs.website_url || 'https://example.com' }}
          
          ## Statistics
          - **Total Pages:** {total_pages}
          - **Total Assets:** {total_assets}
          - **Pages Size:** {total_size / 1024 / 1024:.2f} MB
          - **Max Pages Limit:** ${{ github.event.inputs.max_pages || '50' }}
          
          ## Pages
          """
          
          cursor.execute('SELECT url, title, content_length FROM pages ORDER BY crawled_at DESC LIMIT 20')
          for url, title, size in cursor.fetchall():
            report += f"\n- **[{title or 'No Title'}]({url})** ({size / 1024:.1f} KB)"
          
          with open('CRAWL_REPORT.md', 'w') as f:
            f.write(report)
          
          conn.close()
          EOF
        
        env:
          PYTHONPATH: /home/runner/work/web-crawler/web-crawler
      
      - name: Upload database artifact
        if: success()
        uses: actions/upload-artifact@v3
        with:
          name: crawled-database
          path: crawled.db
          retention-days: 90
      
      - name: Upload report
        if: success()
        uses: actions/upload-artifact@v3
        with:
          name: crawl-report
          path: CRAWL_REPORT.md
          retention-days: 30
      
      - name: Create GitHub Release
        if: success()
        uses: softprops/action-gh-release@v1
        with:
          files: |
            crawled.db
            CRAWL_REPORT.md
          tag_name: crawl-${{ github.run_number }}
          body_path: CRAWL_REPORT.md
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'âœ… Crawl completed! Database artifact uploaded.'
            })
      
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createIssue({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'âŒ Crawl failed',
              body: `Crawler failed for run #${context.runId}. Check logs.`,
              labels: ['bug', 'crawler']
            })
