name: batch-crawl (multiple sites)

on:
  workflow_dispatch:
    inputs:
      sites_json:
        description: 'JSON array of sites [{"url": "...", "max_pages": 50}, ...]'
        required: true
        default: '[{"url": "https://example.com", "max_pages": 50}]'

jobs:
  crawl:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        site: ${{ fromJSON(inputs.sites_json) }}
      max-parallel: 3
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Setup crawl config
        run: |
          python3 << 'PYTHON'
          import json
          import os
          
          site = ${{ toJSON(matrix.site) }}
          url = site.get('url')
          max_pages = site.get('max_pages', 50)
          
          with open('.env', 'w') as f:
            f.write(f"START_URL={url}\n")
            f.write(f"MAX_PAGES={max_pages}\n")
            f.write(f"LOG_LEVEL=INFO\n")
          
          # Generate domain name
          domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('.', '_')
          with open('domain.txt', 'w') as f:
            f.write(domain)
          PYTHON

      - name: Run crawler
        run: |
          domain=$(cat domain.txt)
          python3 smart_archiver_v2.py \
            --url "${{ matrix.site.url }}" \
            --max-pages ${{ matrix.site.max_pages }} \
            --db "${domain}.db"

      - name: Export to WARC
        run: |
          domain=$(cat domain.txt)
          python3 export_to_warc.py "${domain}.db" "${domain}.warc.gz"
        continue-on-error: true

      - name: Export to WACZ
        run: |
          domain=$(cat domain.txt)
          python3 export_to_wacz.py "${domain}.db" "${domain}.wacz"
        continue-on-error: true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: batch-results-${{ matrix.site.url }}
          path: |
            *.db
            *.warc.gz
            *.wacz
          retention-days: 90

  summary:
    needs: crawl
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Create summary
        run: |
          echo "# Batch Crawl Summary" > summary.md
          echo "" >> summary.md
          echo "**Date:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> summary.md
          echo "**Status:** âœ… Completed" >> summary.md
          echo "" >> summary.md
          echo "## Sites Processed" >> summary.md
          echo "- Check Artifacts tab for individual results" >> summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: batch-summary
          path: summary.md
          retention-days: 90
