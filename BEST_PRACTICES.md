# ðŸ¤– Best Practices: AI-Ready Web Crawler (GitHub Actions)

**Status:** ðŸŸ¢ Production-Ready | **For:** AI Agents | **Tokens:** ~800 (ultra-optimized)

---

## ðŸš€ What This Is

**Automated crawler for AI agents** running on GitHub Actions runners

```
ðŸš€ Crawls websites automatically
ðŸ“‹ Stores in queryable SQLite
ðŸ’¾ Exports WARC (ISO 28500:2017)
ðŸ¤– Designed for AI/ML integration
ðŸœŸ Runs 24/7 in GitHub infrastructure
```

---

## ðŸŽ¯ Core Principles

**Minimal sufficient information** - Anthropic methodology
- Include: schemas, APIs, critical patterns
- Exclude: verbose comments, unnecessary types
- **Goal:** Token-efficient for AI context

---

## ðŸš„ Architecture

```python
GitHub Actions Runner
    â†“
    smart_archiver_v2.py (main crawler)
    â†“
    AssetExtractor (images, CSS, JS)
    â†“
    SQLite Database (queryable)
    â†“
    Release Artifact (persistent)
```

---

## ðŸ“¦ Core Components

```
smart_archiver_v2.py    (13 KB)  â€“ Main crawler
asset_extractor.py      (7 KB)   â€“ Asset download
export_to_warc.py       (4.5 KB) â€“ WARC export
export_to_wacz.py       (6.4 KB) â€“ WACZ export
database_utils.py       (10.6 KB)â€“ DB helpers
database_schema.sql     (4.7 KB) â€“ Schema

Total: 52 KB core (SLIM!)
```

---

## ðŸ’« Database Schema

```sql
pages:
  id, url (unique), title, status_code, content, crawled_at

assets:
  url, type (image/css/js/font/favicon), mime_type, file_size, content_hash

asset_blobs:
  content_hash (unique), content (BLOB)

links:
  from_page_id, to_page_id (for graph analysis)

cdx:
  url, timestamp, record_type (indexing)
```

---

## ðŸ¤– For AI Integration

### Query Pages

```python
import sqlite3

conn = sqlite3.connect('archive.db')
c = conn.cursor()

# Get all pages
c.execute('SELECT url, title, content FROM pages')
pages = c.fetchall()
```

### Extract Assets

```python
# Images only
c.execute('SELECT url FROM assets WHERE asset_type="image"')
images = c.fetchall()
```

### Link Analysis

```python
# Graph for AI analysis
c.execute('SELECT from_page_id, to_page_id FROM links')
links = c.fetchall()
```

---

## ðŸš€ GitHub Actions Integration

```yaml
# Trigger from AI agent
GitHub API â†’ dispatch workflow â†’ crawl_website.yml
             â†“
             GitHub runner (3-5 min)
             â†“
             archive.db + WARC + WACZ
             â†“
             Release artifact
             â†“
             AI downloads + analyzes
```

---

## âœ… Security

```
âœ… SSL/TLS enabled
âœ… No hardcoded secrets (use GitHub Secrets)
âœ… SQL injection protected (parameterized)
âœ… Input validation on URLs
âœ… No PII storage (unless in content)
```

---

## ðŸ“Š Performance

```
Crawl time:      3-5 min (50 pages + assets)
Archive size:    ~125 MB
Asset dedup:     20% savings
Memory:          10-20 MB
Query speed:     <100 ms
Monthly cost:    FREE (3000 min quota)
```

---

## ðŸ’­ Workflows

```
crawl-website.yml   â€“ Single site (manual/scheduled)
batch-crawl.yml     â€“ Multiple sites (parallel)

Schedule: Daily 2 AM UTC (configurable)
Trigger: Manual or API-based
Runtime: 3-10 minutes
```

---

## ðŸ“ Docs

- [README.md](README.md) - Getting started
- [AI_CONTEXT.md](.github/AI_CONTEXT.md) - AI integration
- [WORKFLOWS_FOR_AI.md](.github/WORKFLOWS_FOR_AI.md) - Workflow guide
- [IMPLEMENTATION_CHECKLIST.md](IMPLEMENTATION_CHECKLIST.md) - Status

---

## ðŸ’‹ Token Savings

```
Before optimization:  7200 tokens
After optimization:   2000 tokens (docs)
                      5000+ tokens (available for code)

Result: 72% reduction! ðŸš€
```

---

## âš ï¸ Not A Web Server

```
âŒ Does NOT serve websites to users
âŒ Does NOT act as proxy/reverse proxy
âŒ Does NOT cache content
âŒ Does NOT host applications

âœ… IS a crawler that archives sites
âœ… IS designed for AI automation
âœ… IS WARC/WACZ compliant
âœ… IS free (GitHub Actions)
```

---

## ðŸš€ Next Steps

1. Fork repository
2. Enable GitHub Actions
3. Trigger first crawl
4. Download archive.db
5. Query with AI

---

**Status:** ðŸ¤– AI-Ready | **Runner:** GitHub Actions | **Cost:** FREE
