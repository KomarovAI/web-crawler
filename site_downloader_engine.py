#!/usr/bin/env python3
"""
üî• SITE DOWNLOADER ENGINE v3
Production-ready wget/httrack with strict size limits
- Max 200KB per HTML file (callmedley pages are bloated with analytics/tracking)
- Max 100KB per image (no hi-res photos)
- Max 50KB per CSS/JS (minified)
- Strict quota enforcement (hard stop at limit)
"""

import asyncio
import subprocess
import sqlite3
import shutil
import json
from pathlib import Path
from urllib.parse import urlparse
import logging
from datetime import datetime
import hashlib
import time

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

class SiteDownloaderEngine:
    """üî• Download full website using wget with strict size controls"""
    
    # Strict file size limits (from real-world testing with callmedley.com)
    FILE_SIZE_LIMITS = {
        'html': 200 * 1024,      # 200KB max per HTML (callmedley pages: ~350KB bloated)
        'css': 50 * 1024,        # 50KB max per CSS
        'javascript': 100 * 1024, # 100KB max per JS
        'image': 100 * 1024,     # 100KB max per image
        'font': 50 * 1024,       # 50KB max per font
        'other': 50 * 1024,      # 50KB max for everything else
    }
    
    # WGET parameters (optimized from GNU Wget docs)
    WGET_PARAMS = {
        'level': 2,              # 2 levels deep
        'wait': 2,               # 2 sec between requests
        'random_wait': 2,        # +0-2 sec random
        'connect_timeout': 20,
        'read_timeout': 20,
        'retries': 2,            # Less retries
        'limit_rate': 256,       # 256KB/s (conservative)
    }\n    \n    def __init__(self, conn: sqlite3.Connection, domain: str):\n        self.conn = conn\n        self.domain = domain\n        self.file_counter = 0\n        self.max_files_limit = 0\n        self._init_tables()\n    \n    def _init_tables(self):\n        \"\"\"Initialize tables for downloaded site\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Downloaded files with extended metadata\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS downloaded_files (\n                id INTEGER PRIMARY KEY,\n                domain TEXT NOT NULL,\n                file_path TEXT NOT NULL,\n                file_type TEXT NOT NULL,\n                file_size INTEGER,\n                content_hash TEXT UNIQUE,\n                file_content BLOB,\n                url TEXT,\n                download_time REAL,\n                downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Download metadata\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS download_metadata (\n                id INTEGER PRIMARY KEY,\n                domain TEXT UNIQUE NOT NULL,\n                tool_used TEXT,\n                start_url TEXT,\n                download_start TIMESTAMP,\n                download_end TIMESTAMP,\n                total_files INTEGER,\n                total_size INTEGER,\n                success BOOLEAN,\n                error_message TEXT,\n                files_by_type TEXT\n            )\n        ''')\n        \n        self.conn.commit()\n    \n    def check_tools(self) -> str:\n        \"\"\"üîç Check which tool is available\"\"\"\n        if shutil.which('httrack'):\n            logger.info(\"‚úÖ httrack found\")\n            return 'httrack'\n        elif shutil.which('wget'):\n            logger.info(\"‚úÖ wget found\")\n            return 'wget'\n        else:\n            logger.error(\"‚ùå No download tool available!\")\n            return None\n    \n    async def download_with_wget(self, url: str, output_dir: Path, max_pages: int) -> dict:\n        \"\"\"‚ö° Download with wget + strict size control\"\"\"\n        logger.info(f\"‚ö° Starting wget: {url}\")\n        logger.info(f\"üìã Target: {max_pages} pages\")\n        logger.info(f\"üìä File size limits: HTML={self.FILE_SIZE_LIMITS['html']//1024}KB, \"\n                   f\"IMG={self.FILE_SIZE_LIMITS['image']//1024}KB, \"\n                   f\"CSS={self.FILE_SIZE_LIMITS['css']//1024}KB\")\n        \n        try:\n            parsed = urlparse(url)\n            domain = parsed.netloc\n            \n            # Conservative quota: max_pages * 150KB per page average\n            quota_kb = max(max_pages * 150, 500)  # Min 500KB\n            \n            cmd = [\n                'wget',\n                '--recursive',\n                '--level=2',\n                \n                # Only accept useful files (strict)\n                '--accept-regex=\\\\.(html?|css|js|jpg|jpeg|png|gif|webp|svg|woff|woff2|ttf|eot)$',\n                '--reject-regex=\\\\.(webm|mp4|mov|avi|mpeg|mkv|flv|wmv|m4v|ts|mpg|3gp|vob|f4v|wav|mp3|aac|flac|opus|zip|exe|torrent|pdf|doc|docx|xls|xlsx)$',\n                \n                # Critical: Only get CSS/JS/images FOR downloaded pages (not all)\n                '--page-requisites',\n                '--no-remove-listing',\n                \n                '--adjust-extension',\n                '--convert-links',\n                '--restrict-file-names=windows',\n                f'--domains={domain}',\n                '--no-parent',\n                \n                # Delays\n                f'--wait={self.WGET_PARAMS[\"wait\"]}',\n                '--random-wait',\n                '--waitretry=5',\n                \n                # Timeouts\n                f'--connect-timeout={self.WGET_PARAMS[\"connect_timeout\"]}',\n                f'--read-timeout={self.WGET_PARAMS[\"read_timeout\"]}',\n                f'--tries={self.WGET_PARAMS[\"retries\"]}',\n                '--retry-connrefused',\n                \n                # Size control (HARD LIMIT)\n                f'--quota={quota_kb}K',\n                f'--limit-rate={self.WGET_PARAMS[\"limit_rate\"]}k',\n                \n                # Optimization\n                '-N',  # Only newer files\n                '--no-verbose',\n                '--show-progress',\n                \n                # User agent\n                '--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',\n                \n                '-P', str(output_dir),\n                url\n            ]\n            \n            logger.info(f\"üîß wget config: level=2, wait=2s, quota={quota_kb}KB, \"\n                       f\"limit={self.WGET_PARAMS['limit_rate']}KB/s\")\n            \n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            \n            stdout, stderr = await process.communicate()\n            \n            if process.returncode not in [0, 1, 8]:\n                if process.returncode > 2:\n                    error_msg = stderr.decode()[-500:]  # Last 500 chars\n                    logger.error(f\"wget failed (code {process.returncode}): {error_msg}\")\n                    return {'success': False, 'error': error_msg}\n            \n            logger.info(\"‚úÖ wget complete\")\n            return {'success': True, 'output_dir': str(output_dir)}\n            \n        except Exception as e:\n            logger.error(f\"wget error: {e}\")\n            return {'success': False, 'error': str(e)}\n    \n    async def download_site(self, url: str, max_pages: int = 100) -> dict:\n        \"\"\"üî• Download website\"\"\"\n        logger.info(f\"\\n{'='*70}\")\n        logger.info(f\"üî• WEBSITE DOWNLOAD: {url}\")\n        logger.info(f\"{'='*70}\")\n        \n        # Validate\n        if not isinstance(max_pages, int) or max_pages < 1 or max_pages > 500:\n            logger.error(f\"‚ùå Invalid max_pages: {max_pages}\")\n            return {'success': False, 'error': 'max_pages must be 1-500'}\n        \n        self.max_files_limit = max_pages * 15  # ~15 assets per page max\n        \n        # Check tools\n        tool = self.check_tools()\n        if not tool:\n            return {'success': False, 'error': 'No download tool available'}\n        \n        # Create temp dir\n        temp_dir = Path('temp_download')\n        temp_dir.mkdir(exist_ok=True)\n        \n        start_time = datetime.now()\n        \n        # Download\n        result = await self.download_with_wget(url, temp_dir, max_pages)\n        \n        if not result['success']:\n            return result\n        \n        # Process files\n        logger.info(\"üì¶ Processing files...\")\n        files_processed = await self._process_and_save_files(temp_dir, url)\n        \n        # Save metadata\n        cursor = self.conn.cursor()\n        stats = self.get_stats()\n        \n        cursor.execute('''\n            INSERT OR REPLACE INTO download_metadata\n            (domain, tool_used, start_url, download_start, download_end, \n             total_files, total_size, success, files_by_type)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            self.domain,\n            tool,\n            url,\n            start_time.isoformat(),\n            datetime.now().isoformat(),\n            files_processed,\n            stats.get('total_size_bytes', 0),\n            True,\n            json.dumps(stats.get('by_type', {}))\n        ))\n        self.conn.commit()\n        \n        # Cleanup\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        \n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n        \n        logger.info(f\"\\n{'='*70}\")\n        logger.info(f\"‚úÖ COMPLETE in {duration:.1f}s\")\n        logger.info(f\"{'='*70}\")\n        logger.info(f\"üìÅ Files: {files_processed}\")\n        logger.info(f\"üíæ Size: {stats.get('total_size_mb', 0):.2f}MB\")\n        logger.info(f\"üìä By type: {stats.get('by_type', {})}\")\n        logger.info(f\"{'='*70}\\n\")\n        \n        return {\n            'success': True,\n            'tool': tool,\n            'files_processed': files_processed,\n            'url': url,\n            'max_pages': max_pages,\n            'duration_seconds': duration,\n            'total_size_mb': stats.get('total_size_mb', 0)\n        }\n    \n    async def _process_and_save_files(self, base_dir: Path, start_url: str) -> int:\n        \"\"\"üì¶ Process files with strict size checks\"\"\"\n        cursor = self.conn.cursor()\n        files_count = 0\n        skipped = 0\n        \n        for file_path in sorted(base_dir.rglob('*')):\n            if file_path.is_file():\n                if files_count >= self.max_files_limit:\n                    logger.warning(f\"‚ö†Ô∏è  File limit reached ({self.max_files_limit})\")\n                    break\n                \n                try:\n                    # Get file type\n                    suffix = file_path.suffix.lower()\n                    if suffix in ['.html', '.htm']:\n                        file_type = 'html'\n                    elif suffix == '.css':\n                        file_type = 'css'\n                    elif suffix == '.js':\n                        file_type = 'javascript'\n                    elif suffix in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:\n                        file_type = 'image'\n                    elif suffix in ['.woff', '.woff2', '.ttf', '.eot']:\n                        file_type = 'font'\n                    else:\n                        file_type = 'other'\n                    \n                    # Read file\n                    start = time.time()\n                    with open(file_path, 'rb') as f:\n                        content = f.read()\n                    download_time = time.time() - start\n                    \n                    file_size = len(content)\n                    \n                    # ‚úÖ STRICT SIZE CHECK\n                    max_size = self.FILE_SIZE_LIMITS.get(file_type, self.FILE_SIZE_LIMITS['other'])\n                    if file_size > max_size:\n                        logger.warning(f\"‚ö†Ô∏è  {file_path.name} too large ({file_size//1024}KB > {max_size//1024}KB limit)\")\n                        skipped += 1\n                        continue\n                    \n                    # Calculate hash\n                    content_hash = hashlib.sha256(content).hexdigest()\n                    rel_path = str(file_path.relative_to(base_dir))\n                    \n                    # Save to DB\n                    cursor.execute('''\n                        INSERT OR IGNORE INTO downloaded_files\n                        (domain, file_path, file_type, file_size, content_hash, \n                         file_content, url, download_time)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                    ''', (\n                        self.domain,\n                        rel_path,\n                        file_type,\n                        file_size,\n                        content_hash,\n                        content,\n                        start_url,\n                        download_time\n                    ))\n                    \n                    files_count += 1\n                    if files_count % 50 == 0:\n                        logger.info(f\"üì¶ Processed {files_count} files...\")\n                    \n                except Exception as e:\n                    logger.debug(f\"Skip {file_path}: {e}\")\n                    skipped += 1\n        \n        self.conn.commit()\n        if skipped > 0:\n            logger.info(f\"‚ÑπÔ∏è  Skipped {skipped} files (oversized)\")\n        return files_count\n    \n    def get_file(self, file_path: str) -> bytes:\n        \"\"\"üìÑ Get file from DB\"\"\"\n        cursor = self.conn.cursor()\n        cursor.execute(\n            'SELECT file_content FROM downloaded_files WHERE domain = ? AND file_path = ?',\n            (self.domain, file_path)\n        )\n        row = cursor.fetchone()\n        return row[0] if row else None\n    \n    def get_stats(self) -> dict:\n        \"\"\"üìä Get stats\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('SELECT COUNT(*) FROM downloaded_files WHERE domain = ?', (self.domain,))\n        total_files = cursor.fetchone()[0]\n        \n        cursor.execute('SELECT SUM(file_size) FROM downloaded_files WHERE domain = ?', (self.domain,))\n        total_size = cursor.fetchone()[0] or 0\n        \n        cursor.execute('''\n            SELECT file_type, COUNT(*), SUM(file_size) FROM downloaded_files \n            WHERE domain = ? GROUP BY file_type\n        ''', (self.domain,))\n        \n        by_type = {}\n        for file_type, count, size in cursor.fetchall():\n            by_type[file_type] = {\n                'count': count,\n                'size_mb': size / 1024 / 1024 if size else 0\n            }\n        \n        return {\n            'total_files': total_files,\n            'total_size_bytes': total_size,\n            'total_size_mb': total_size / 1024 / 1024,\n            'by_type': by_type\n        }\n\n\nif __name__ == '__main__':\n    import sys\n    \n    url = sys.argv[1] if len(sys.argv) > 1 else 'https://example.com'\n    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 100\n    domain = urlparse(url).netloc\n    \n    conn = sqlite3.connect('archive.db')\n    downloader = SiteDownloaderEngine(conn, domain)\n    \n    result = asyncio.run(downloader.download_site(url, max_pages))\n    \n    if result['success']:\n        print(f\"\\n{'='*70}\")\n        print(\"‚úÖ SUCCESS\")\n        print(f\"{'='*70}\")\n        print(f\"Files: {result['files_processed']}\")\n        print(f\"Size: {result['total_size_mb']:.2f}MB\")\n        print(f\"Duration: {result['duration_seconds']:.1f}s\")\n        print(f\"{'='*70}\")\n    else:\n        print(f\"\\n‚ùå Failed: {result['error']}\")\n    \n    conn.close()\n"