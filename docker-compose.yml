version: '3.8'

# Optimized docker-compose for VPS production deployment
# Multi-service setup with health checks, logging, resource limits

services:
  # Service 1: Basic crawler (fast, database storage)
  crawler:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - crawler:latest
    container_name: web-crawler-basic
    env_file: .env
    environment:
      - START_URL=${START_URL:-https://example.com}
      - MAX_PAGES=${MAX_PAGES:-50}
      - TIMEOUT=${TIMEOUT:-10}
      - USE_DB=${USE_DB:-true}
      - DB_FILE=${DB_FILE:-crawled.db}
    volumes:
      # Output and database persistence
      - crawler_db:/app
      - crawler_output:/app/output
    networks:
      - crawler_net
    # Resource limits for VPS stability
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    # Health check
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: on-failure:3
    command: python crawler.py

  # Service 2: Full archiver (complete site backup)
  crawler-full:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - crawler:latest
    container_name: web-crawler-full
    env_file: .env
    environment:
      - START_URL=${START_URL:-https://example.com}
      - MAX_PAGES=${MAX_PAGES:-20}
      - TIMEOUT=${TIMEOUT:-10}
      - OUTPUT_DIR=${OUTPUT_DIR:-site_archive}
    volumes:
      # Site archive persistence
      - crawler_archive:/app/site_archive
      - crawler_output:/app/output
    networks:
      - crawler_net
    # Resource limits (more restrictive for full archiver)
    deploy:
      resources:
        limits:
          cpus: '0.8'
          memory: 1024M
        reservations:
          cpus: '0.4'
          memory: 512M
    # Health check
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "3"
    restart: on-failure:2
    # Don't start by default (resource intensive)
    profiles:
      - archiver
    command: python crawler_full.py

  # Optional: Nginx reverse proxy for web access
  nginx:
    image: nginx:alpine
    container_name: web-crawler-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - crawler_archive:/usr/share/nginx/html/archive:ro
      - ./ssl:/etc/nginx/ssl:ro
    networks:
      - crawler_net
    depends_on:
      - crawler
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"
    profiles:
      - web

volumes:
  # Persistent storage for crawler database
  crawler_db:
    driver: local
  # Persistent storage for site archives
  crawler_archive:
    driver: local
  # Output files
  crawler_output:
    driver: local

networks:
  # Custom bridge network for service communication
  crawler_net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# Usage:
# Basic crawler: docker-compose up crawler
# Full archiver: docker-compose --profile archiver up crawler-full
# With web UI: docker-compose --profile web up
# All together: docker-compose --profile archiver --profile web up
