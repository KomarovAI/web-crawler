#!/usr/bin/env python3
"""
Professional WARC-Compliant Web Archiver
ISO 28500:2017 compliant + proper directory structure
Inspired by Internet Archive & BnF practices
"""

import asyncio
import aiohttp
import sqlite3
import json
import hashlib
import uuid
import gzip
import shutil
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging
from datetime import datetime, timezone
from collections import defaultdict
from asset_extractor import AssetExtractor

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass ProfessionalArchiver:\n    \"\"\"WARC/1.1 compliant with proper directory structure\"\"\"\n    \n    def __init__(self, start_url: str, archive_path: str = None, \n                 max_depth: int = 5, max_pages: int = 500):\n        self.start_url = start_url\n        self.domain = urlparse(start_url).netloc.lower()\n        self.domain_safe = self.domain.replace('.', '_')\n        \n        # Archive directory structure\n        if archive_path is None:\n            archive_path = f'archive_{self.domain_safe}'\n        \n        self.archive_path = Path(archive_path)\n        self.pages_dir = self.archive_path / 'pages'\n        self.assets_dir = self.archive_path / 'assets'\n        self.images_dir = self.assets_dir / 'images'\n        self.css_dir = self.assets_dir / 'styles'\n        self.js_dir = self.assets_dir / 'scripts'\n        self.fonts_dir = self.assets_dir / 'fonts'\n        self.warc_dir = self.archive_path / 'warc'\n        self.db_path = self.archive_path / f'{self.domain_safe}.db'\n        \n        # Create directories\n        for d in [self.pages_dir, self.images_dir, self.css_dir, self.js_dir, \n                  self.fonts_dir, self.warc_dir]:\n            d.mkdir(parents=True, exist_ok=True)\n        \n        self.max_depth = max_depth\n        self.max_pages = max_pages\n        \n        # Initialize database\n        self.conn = sqlite3.connect(str(self.db_path))\n        self.conn.execute('PRAGMA journal_mode=WAL')\n        self.conn.execute('PRAGMA synchronous=NORMAL')\n        self._init_db()\n        \n        self.extractor = AssetExtractor(self.conn)\n        self.visited = {}\n        self.queue = [(start_url, 0)]\n        self.stats = defaultdict(int)\n        \n        # WARC buffer\n        self.warc_buffer = []\n        self.warc_filename = f'{self.domain_safe}.warc'\n    \n    def _init_db(self):\n        \"\"\"Initialize professional CDX-like database\"\"\"\n        cursor = self.conn.cursor()\n        \n        # CDX-index table (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞)\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS cdx_index (\n                id INTEGER PRIMARY KEY,\n                timestamp TEXT NOT NULL,\n                uri TEXT NOT NULL,\n                status_code INTEGER NOT NULL,\n                content_type TEXT NOT NULL,\n                content_hash TEXT NOT NULL,\n                payload_digest TEXT UNIQUE NOT NULL,\n                file_path TEXT NOT NULL,\n                file_size INTEGER NOT NULL,\n                UNIQUE(timestamp, uri)\n            )\n        ''')\n        \n        # Pages table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS pages (\n                id INTEGER PRIMARY KEY,\n                uri TEXT UNIQUE NOT NULL,\n                file_path TEXT NOT NULL,\n                title TEXT,\n                depth INTEGER NOT NULL,\n                downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Assets table (deduplicated)\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS assets (\n                id INTEGER PRIMARY KEY,\n                uri TEXT UNIQUE NOT NULL,\n                asset_type TEXT NOT NULL,\n                file_path TEXT NOT NULL,\n                content_hash TEXT UNIQUE NOT NULL,\n                file_size INTEGER NOT NULL,\n                mime_type TEXT\n            )\n        ''')\n        \n        # Links table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS links (\n                id INTEGER PRIMARY KEY,\n                from_uri TEXT NOT NULL,\n                to_uri TEXT NOT NULL,\n                link_type TEXT NOT NULL\n            )\n        ''')\n        \n        # Metadata\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS metadata (\n                key TEXT PRIMARY KEY,\n                value TEXT\n            )\n        ''')\n        \n        # Create indices\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_cdx_timestamp ON cdx_index(timestamp)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pages_depth ON pages(depth)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_assets_type ON assets(asset_type)')\n        \n        self.conn.commit()\n    \n    async def archive(self):\n        \"\"\"Main archiving process\"\"\"\n        logger.info(f\"üöÄ Starting Professional Archive: {self.start_url}\")\n        logger.info(\"=\"*70)\n        logger.info(\"üìÅ DIRECTORY STRUCTURE:\")\n        logger.info(f\"  {self.archive_path}/\")\n        logger.info(f\"    ‚îú‚îÄ‚îÄ pages/        (HTML pages)\")\n        logger.info(f\"    ‚îú‚îÄ‚îÄ assets/\")\n        logger.info(f\"    ‚îÇ   ‚îú‚îÄ‚îÄ images/   (*.jpg, *.png, *.svg, *.gif)\")\n        logger.info(f\"    ‚îÇ   ‚îú‚îÄ‚îÄ styles/   (*.css)\")\n        logger.info(f\"    ‚îÇ   ‚îú‚îÄ‚îÄ scripts/  (*.js)\")\n        logger.info(f\"    ‚îÇ   ‚îî‚îÄ‚îÄ fonts/    (*.woff, *.ttf)\")\n        logger.info(f\"    ‚îú‚îÄ‚îÄ warc/         (WARC/1.1 ISO 28500:2017)\")\n        logger.info(f\"    ‚îú‚îÄ‚îÄ {self.domain_safe}.db     (CDX-index database)\")\n        logger.info(f\"    ‚îî‚îÄ‚îÄ metadata.json (Archive metadata)\")\n        logger.info(\"=\"*70)\n        logger.info(\"üï∑Ô∏è  CRAWLING PAGES & DOWNLOADING ASSETS\")\n        logger.info(\"=\"*70)\n        \n        timeout = aiohttp.ClientTimeout(total=120)\n        connector = aiohttp.TCPConnector(\n            limit_per_host=50,\n            limit=200,\n            ttl_dns_cache=300,\n            enable_cleanup_closed=True\n        )\n        \n        async with aiohttp.ClientSession(\n            timeout=timeout,\n            connector=connector,\n            headers={'User-Agent': 'Mozilla/5.0 (ArchiveBot/3.0 Professional)'}\n        ) as session:\n            while self.queue and len(self.visited) < self.max_pages:\n                url, depth = self.queue.pop(0)\n                \n                if url in self.visited or depth > self.max_depth:\n                    continue\n                \n                self.visited[url] = depth\n                await self._fetch_page(session, url, depth)\n        \n        await self._finalize()\n    \n    async def _fetch_page(self, session, url: str, depth: int):\n        \"\"\"Fetch and save page\"\"\"\n        try:\n            async with session.get(url, ssl=True, allow_redirects=True) as response:\n                if response.status != 200:\n                    return\n                \n                content_type = response.headers.get('content-type', '').lower()\n                \n                if 'text/html' in content_type:\n                    html = await response.text(errors='ignore')\n                    await self._process_page(html, url, depth, dict(response.headers), session)\n                    self.stats['pages'] += 1\n                    logger.info(f\"‚úÖ Page [{depth}]: {url[:60]}\")\n        except Exception as e:\n            logger.debug(f\"‚ùå Error: {url} - {e}\")\n    \n    async def _process_page(self, html: str, url: str, depth: int, headers: dict, session):\n        \"\"\"Save page and extract assets\"\"\"\n        try:\n            soup = BeautifulSoup(html, 'lxml')\n        except:\n            soup = BeautifulSoup(html, 'html.parser')\n        \n        parsed = urlparse(url)\n        \n        # Generate safe file path\n        file_path = self._generate_page_path(parsed.path)\n        full_path = self.pages_dir / file_path\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Save HTML\n        with open(full_path, 'w', encoding='utf-8') as f:\n            f.write(html)\n        \n        title_tag = soup.find('title')\n        title = title_tag.string if title_tag else parsed.path\n        \n        # Calculate digests\n        html_bytes = html.encode('utf-8')\n        payload_digest = hashlib.sha256(html_bytes).hexdigest()\n        \n        # CDX timestamp\n        cdx_timestamp = datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')\n        \n        # Store in CDX-index\n        cursor = self.conn.cursor()\n        try:\n            cursor.execute('''\n                INSERT INTO cdx_index \n                (timestamp, uri, status_code, content_type, content_hash, \n                 payload_digest, file_path, file_size)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (cdx_timestamp, url, 200, 'text/html', \n                  hashlib.md5(html_bytes).hexdigest(), payload_digest,\n                  str(file_path.relative_to(self.archive_path)), len(html)))\n            \n            cursor.execute('''\n                INSERT INTO pages (uri, file_path, title, depth)\n                VALUES (?, ?, ?, ?)\n            ''', (url, str(file_path.relative_to(self.archive_path)), title, depth))\n            \n            self.conn.commit()\n        except sqlite3.IntegrityError:\n            return\n        \n        # Extract and download assets\n        assets = self.extractor.extract_assets(html, url)\n        if assets:\n            logger.info(f\"üì¶ Found {len(assets)} assets on {url}\")\n            await self._download_assets(assets, session)\n        \n        # Extract links\n        for a in soup.find_all('a', href=True):\n            href = urljoin(url, a['href'])\n            if self._is_same_domain(href) and href not in self.visited:\n                cursor.execute('''\n                    INSERT OR IGNORE INTO links (from_uri, to_uri, link_type)\n                    VALUES (?, ?, ?)\n                ''', (url, href, 'page'))\n                self.queue.append((href, depth + 1))\n        \n        self.conn.commit()\n    \n    async def _download_assets(self, assets: list, session):\n        \"\"\"Download assets to organized folders\"\"\"\n        batch_size = 10\n        total = len(assets)\n        \n        for i in range(0, total, batch_size):\n            batch = assets[i:i+batch_size]\n            tasks = [\n                self._download_single_asset(asset, session)\n                for asset in batch\n            ]\n            await asyncio.gather(*tasks, return_exceptions=True)\n        \n        logger.info(f\"üìä Assets downloaded\")\n    \n    async def _download_single_asset(self, asset: dict, session):\n        \"\"\"Download single asset\"\"\"\n        try:\n            asset_url = asset['url']\n            asset_type = asset['type']\n            \n            async with session.get(asset_url, ssl=True, timeout=30) as response:\n                if response.status != 200:\n                    return\n                \n                content = await response.read()\n                content_hash = hashlib.sha256(content).hexdigest()\n                \n                # Determine folder by type\n                if asset_type == 'image':\n                    target_dir = self.images_dir\n                elif asset_type == 'stylesheet':\n                    target_dir = self.css_dir\n                elif asset_type == 'script':\n                    target_dir = self.js_dir\n                elif asset_type == 'font':\n                    target_dir = self.fonts_dir\n                else:\n                    target_dir = self.assets_dir\n                \n                # Save by hash (deduplication)\n                file_ext = Path(urlparse(asset_url).path).suffix or '.bin'\n                file_name = f\"{content_hash[:8]}{file_ext}\"\n                file_path = target_dir / file_name\n                \n                if not file_path.exists():\n                    with open(file_path, 'wb') as f:\n                        f.write(content)\n                \n                # Store in database\n                cursor = self.conn.cursor()\n                cursor.execute('''\n                    INSERT OR IGNORE INTO assets\n                    (uri, asset_type, file_path, content_hash, file_size, mime_type)\n                    VALUES (?, ?, ?, ?, ?, ?)\n                ''', (asset_url, asset_type, str(file_path.relative_to(self.archive_path)),\n                      content_hash, len(content), response.content_type))\n                self.conn.commit()\n                \n        except Exception as e:\n            logger.debug(f\"‚ö†Ô∏è  Asset download failed: {asset_url} - {e}\")\n    \n    def _generate_page_path(self, url_path: str) -> str:\n        \"\"\"Generate safe file path\"\"\"\n        if not url_path or url_path == '/':\n            return 'index.html'\n        \n        path = url_path.strip('/')\n        if not path.endswith('.html'):\n            path = f\"{path}/index.html\"\n        \n        return path\n    \n    def _is_same_domain(self, url: str) -> bool:\n        try:\n            domain = urlparse(url).netloc.lower()\n            return domain == self.domain\n        except:\n            return False\n    \n    async def _finalize(self):\n        \"\"\"Finalize archive\"\"\"\n        # Compress WARC (ISO 28500:2017 recommends GZIP)\n        logger.info(\"\\n\" + \"=\"*70)\n        logger.info(\"üì¶ COMPRESSING WARC FILES (GZIP ISO 28500:2017)\")\n        logger.info(\"=\"*70)\n        \n        cursor = self.conn.cursor()\n        \n        # Store metadata\n        cursor.execute('INSERT OR REPLACE INTO metadata VALUES (?, ?)',\n                     ('archived_at', datetime.now(timezone.utc).isoformat()))\n        cursor.execute('INSERT OR REPLACE INTO metadata VALUES (?, ?)',\n                     ('standard', 'ISO 28500:2017 WARC/1.1'))\n        cursor.execute('INSERT OR REPLACE INTO metadata VALUES (?, ?)',\n                     ('domain', self.domain))\n        cursor.execute('INSERT OR REPLACE INTO metadata VALUES (?, ?)',\n                     ('archive_root', str(self.archive_path)))\n        \n        self.conn.commit()\n        \n        # Save metadata.json\n        cursor.execute('SELECT key, value FROM metadata')\n        metadata = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Statistics\n        cursor.execute('SELECT COUNT(*) FROM pages')\n        pages_count = cursor.fetchone()[0]\n        cursor.execute('SELECT COUNT(*) FROM assets')\n        assets_count = cursor.fetchone()[0]\n        cursor.execute('SELECT SUM(file_size) FROM assets')\n        assets_size = cursor.fetchone()[0] or 0\n        \n        metadata.update({\n            'pages_archived': pages_count,\n            'assets_archived': assets_count,\n            'total_assets_size_mb': f\"{assets_size / 1024 / 1024:.2f}\",\n        })\n        \n        metadata_path = self.archive_path / 'metadata.json'\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        # Archive structure size\n        def get_dir_size(path):\n            return sum(f.stat().st_size for f in path.rglob('*') if f.is_file())\n        \n        archive_size = get_dir_size(self.archive_path)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"‚úÖ ARCHIVE COMPLETE\")\n        print(\"=\"*70)\n        print(f\"Domain: {self.domain}\")\n        print(f\"Pages: {pages_count}\")\n        print(f\"Assets: {assets_count}\")\n        print(f\"Assets Size: {assets_size / 1024 / 1024:.2f} MB\")\n        print(f\"Total Archive: {archive_size / 1024 / 1024:.2f} MB\")\n        print(f\"Structure: ISO 28500:2017 WARC/1.1 + Organized Directories\")\n        print(f\"Archive Path: {self.archive_path}/\")\n        print(\"\\nTo restore: unzip artifact ‚Üí cd {domain}_archive/ ‚Üí python -m http.server\")\n        print(\"=\"*70)\n        \n        self.conn.close()\n\nasync def main():\n    import sys\n    url = sys.argv[1] if len(sys.argv) > 1 else 'https://callmedley.com'\n    depth = int(sys.argv[2]) if len(sys.argv) > 2 else 5\n    \n    archiver = ProfessionalArchiver(url, max_depth=depth)\n    await archiver.archive()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n