# ğŸ¤– Web Crawler for AI Agents (GitHub Actions)

**Purpose:** ğŸ§  AI/ML agents archive websites via GitHub Actions runners  
**Status:** âœ… Production Ready | **Standard:** ISO 28500:2017  
**Auto-Execute:** GitHub Actions scheduled crawls | **Output:** WARC + WACZ  

âš ï¸ **THIS IS A CRAWLER FOR AI AGENTS, NOT A WEB HOSTING PLATFORM**

---

## ğŸ¯ What This Does

âœ… **Automated website crawling** via GitHub Actions runners  
âœ… **Creates WARC archives** (ISO 28500:2017 compliant)  
âœ… **Extracts assets** (images, CSS, JS, fonts)  
âœ… **Generates WACZ** (browser-playable packages)  
âœ… **Stores in SQLite** (queryable database)  
âœ… **Zero manual intervention** - scheduled + on-demand  
**âœ… NEW:** **âš¡ Ultra-fast website downloader (HTTrack, WGET, Monolith)**  

âŒ **NOT:** Web server, hosting platform, or reverse proxy  
âŒ **NOT:** For serving websites to users  
âŒ **NOT:** A cache/CDN  

---

## ğŸš€ NEW: ULTIMATE WEBSITE DOWNLOADER

### ğŸ”¥ Download ANY website in 30 seconds!

#### Quick Examples:

```bash
# HTTrack (recommended - maximum control)
httrack https://callmedley.com -O ./site -k -%e -c16 --max-rate=0

# WGET (built-in - ultra-fast)
wget -m -p -k --domains callmedley.com --no-parent https://callmedley.com/

# Monolith (single file - easy sharing)
monolith https://callmedley.com/ -o site.html

# Docker (no installation needed)
docker run -v $(pwd)/downloads:/app/downloads downloader \
  download https://callmedley.com httrack

# Python module
python3 downloader/site_downloader.py https://callmedley.com -m all

# CLI Script
./downloader/cli.sh download https://callmedley.com all
```

### ğŸ“š Three Powerful Engines

| Engine | Speed | Control | Install | Best For |
|--------|-------|---------|---------|----------|
| **HTTrack** â­ | âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡âš¡ | `brew install` | Maximum control + offline |
| **WGET** âš¡ | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ | Built-in | Raw speed |
| **Monolith** ğŸ“¦ | âš¡âš¡âš¡ | âš¡ | `brew install` | Single HTML file |

### ğŸ Includes Everything

âœ… CLI script (bash)  
âœ… Python module  
âœ… Docker container  
âœ… GitHub Actions workflow  
âœ… Full documentation  

ğŸ‘‰ **[START HERE: downloader/QUICKSTART.md](downloader/QUICKSTART.md)**  
ğŸ“– **[FULL DOCS: downloader/README.md](downloader/README.md)**  

---

## ğŸš€ How It Works (Crawler)

### Via GitHub Actions (Automated)

```yaml
# Scheduled daily at 2 AM UTC
# Crawls configured websites
# Generates archives automatically
# Stores artifacts for 90 days
```

### Via CLI (Manual)

```bash
python3 smart_archiver_v2.py https://example.com 5
```

---

## ğŸ¤– For AI Agents

This repo is **AI-agent-friendly**:

```
âœ… Token-optimized docs (2000 tokens)
âœ… Modular code (easy to fork/extend)
âœ… Clear API (simple Python interface)
âœ… Well-documented (easy to understand)
âœ… Production-ready (battle-tested)
âœ… Downloader module (ultra-fast site extraction)
```

**Use Case:** Train AI to crawl, archive, download, and analyze websites autonomously.

---

## âš¡ GitHub Actions Runner Features

```
âœ… Scheduled crawls (daily, weekly, custom)
âœ… On-demand manual triggers
âœ… Parallel multi-site crawling
âœ… Auto-generated releases
âœ… Artifact storage (90 days)
âœ… Free tier: 3000 min/month (we use ~150 min)
âœ… Website downloads via workflow
```

### Example: Daily Archive

```bash
# Every day at 2 AM UTC
# Crawls example.com (5 levels deep)
# Creates archive.db (~125 MB)
# Exports to WARC + WACZ
# Stores as release artifact
```

---

## ğŸ“¦ Core (52 KB Slim Code)

| File | Purpose | For AI |
|------|---------|--------|
| smart_archiver_v2.py | Main crawler | Easy to fork/customize |
| asset_extractor.py | Asset download | Modular, reusable |
| export_to_warc.py | Format conversion | Standard output |
| export_to_wacz.py | Playable package | Shareable archive |
| database_utils.py | DB helpers | Query interface |
| **downloader/** | **New: Fast downloader** | **3 engines, CLI + Python** |

---

## ğŸš€ Quick Start (For AI Automation)

### 1. Fork This Repo

```bash
git clone https://github.com/YOUR-USERNAME/web-crawler
cd web-crawler
```

### 2. Configure GitHub Secrets

```bash
# .github/workflows/crawl-website.yml
env:
  TARGET_URL: https://your-site.com
  MAX_DEPTH: 5
```

### 3. Enable Actions

```
Settings â†’ Actions â†’ Allow all actions â†’ Save
```

### 4. Trigger Crawl

```
Actions â†’ crawl-website â†’ Run workflow
```

### 5. Download Archive

```
Releases â†’ Latest â†’ Download archive.db / .wacz
```

---

## ğŸ§  API for AI Agents

```python
from smart_archiver_v2 import WARCCompliantArchiver
import asyncio

async def crawl_for_ai(url: str):
    archiver = WARCCompliantArchiver(
        start_url=url,
        db_path='archive.db',
        max_depth=5,
        max_pages=500
    )
    await archiver.archive()
    return 'archive.db'

# Use in AI agent
db = asyncio.run(crawl_for_ai('https://example.com'))
```

### Download Sites Programmatically

```python
from downloader.site_downloader import SiteDownloader

# Fast download with Python
downloader = SiteDownloader()
result = downloader.download('https://example.com', method='httrack')
# Or use all three methods
results = downloader.download_all('https://example.com')
```

---

## ğŸ’¾ Outputs

```
archive.db        SQLite (queryable by AI)
archive.warc.gz   ISO 28500:2017 standard
archive.wacz      Browser-playable
downloads/        Full website copies (HTTrack/WGET)
site.html         Single-file archive (Monolith)
```

**For AI:** Query SQLite directly

```sql
SELECT url, title FROM pages WHERE domain = 'example.com';
SELECT url, asset_type FROM assets WHERE asset_type = 'image';
```

---

## ğŸ” Security

âœ… SSL/TLS enabled (no MITM)  
âœ… No secrets in repo (use GitHub Secrets)  
âœ… No hardcoded credentials  
âœ… Input validation on URLs  
âœ… SQL injection protected  

---

## ğŸ“Š Performance

```
Crawl time:      3-4 minutes (50 pages + assets)
Download time:   1-5 minutes (full site with HTTrack)
Archive size:    ~125 MB
Asset dedup:     20% storage savings
Memory:          10-20 MB
Query speed:     <100ms
```

---

## ğŸ”§ For AI Development

### Fork & Customize

```bash
# Add AI-specific features
git checkout -b feature/ai-analysis

# Example: Add sentiment analysis to crawled content
# Example: Add NLP entity extraction
# Example: Add image classification
```

### Extend API

```python
# Add to smart_archiver_v2.py
class AIArchiver(WARCCompliantArchiver):
    async def analyze_content(self):
        # AI analysis here
        pass
```

---

## ğŸ“– Docs

- [ğŸš€ DOWNLOADER QUICKSTART](downloader/QUICKSTART.md) - **30 seconds to download any site**
- [ğŸ“š DOWNLOADER FULL DOCS](downloader/README.md) - Complete documentation
- [BEST_PRACTICES.md](BEST_PRACTICES.md) - Architecture
- [IMPLEMENTATION_CHECKLIST.md](IMPLEMENTATION_CHECKLIST.md) - Tracking
- [.env.example](.env.example) - Config

---

## âœ¨ Tech Stack

```
Python 3.11+
aiohttp (async HTTP)
beautifulsoup4 (HTML parsing)
SQLite3 (database)
Docker (containerization)
GitHub Actions (CI/CD)

âœ… NEW: HTTrack, WGET, Monolith (downloaders)
```

---

## â±ï¸ GitHub Actions Usage

```
Free tier:  3000 min/month
Our usage:  ~150 min/month (5%)
Cost:       FREE
```

---

## âš ï¸ Important: This Is NOT

```
âŒ Web server (doesn't serve content)
âŒ Reverse proxy (not a middleman)
âŒ Web hosting (archives only, no live serving)
âŒ API provider (internal use only)
âŒ Content delivery (for archival, not distribution)
```

**It's a crawler that runs in GitHub infrastructure + fast local downloader.**

---

## ğŸš€ Next Steps

1. **Quick download?** â†’ [downloader/QUICKSTART.md](downloader/QUICKSTART.md)
2. Fork the repo
3. Enable GitHub Actions
4. Configure target URL
5. Run automated crawls
6. Download sites locally
7. Integrate with your AI agent
8. Analyze archives

---

**Status:** âœ… Production Ready | **For:** AI Agents | **Via:** GitHub Actions + Local Downloader  
**Repo Size:** 60 KB total | **Code:** 52 KB slim | **Docs:** Token-optimized  
**NEW:** âš¡ **Downloader module with 3 powerful engines (HTTrack, WGET, Monolith)**
